{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "antique-letters",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/home/ec2-user/')\n",
    "import membership_inference_attacks2\n",
    "\n",
    "# baseline cnn model for fashion mnist\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "#from matplotlib import pyplot\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import  Lambda\n",
    "#######from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tensorflow.keras import backend as K\n",
    "#from tensorflow.keras.layers import Activation\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from membership_inference_attacks2 import black_box_benchmarks\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from skimage import data, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import cv2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from random import randint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "ALL_XTRAIN, ALL_YTRAIN, ALL_XTEST, ALL_YTEST, ALL_TRAIN_INDEX, ALL_TEST_INDEX =[],[],[],[],[],[]\n",
    "KNN_OUT, KNN_NOOUT, DIST_OUT, DIST_NOOUT, CLASS_IDX, HIGH_ACTIVATION, LOW_ACTIVATION =[], [], [], [], [], [], []\n",
    "#EPS=1\n",
    "#fname=\"MNIST\"\n",
    "#img_path=\"C:/Users/senn/Desktop/test/MEMIMG/\"\n",
    "img_path=\"/home/ec2-user/img/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "concrete-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and test dataset\n",
    "def load_dataset_full(dataset, dnn_type=None):\n",
    "\n",
    "    # load dataset\n",
    "    \n",
    "    if dataset==\"MNIST\":\n",
    "        (trainX, trainY), (testX, testY) = mnist.load_data() \n",
    "        \n",
    "        dim=28\n",
    "           \n",
    "        #concat\n",
    "        trainX = np.concatenate((trainX, testX))\n",
    "        trainY = np.concatenate((trainY, testY))\n",
    "        \n",
    "        #num of classes\n",
    "        num_classes=len(np.unique(trainY))\n",
    "        \n",
    "    \t# reshape dataset to have a single channel\n",
    "        trainX = trainX.reshape(trainX.shape[0], dim, dim, 1)\n",
    "\n",
    "    elif dataset==\"FMNIST\":\n",
    "        (trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
    "        \n",
    "        dim=28\n",
    "                  \n",
    "        #concat\n",
    "        trainX = np.concatenate((trainX, testX))\n",
    "        trainY = np.concatenate((trainY, testY))\n",
    "        \n",
    "        #num of classes\n",
    "        num_classes=len(np.unique(trainY))\n",
    "        \n",
    "    \t# reshape dataset to have a single channel\n",
    "        trainX = trainX.reshape(trainX.shape[0], dim, dim, 1)\n",
    "\n",
    "    elif dataset==\"cifar10\":\n",
    "        (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "\n",
    "        dim=32\n",
    "        #concat\n",
    "        trainX = np.concatenate((trainX, testX))\n",
    "        trainY = np.concatenate((trainY, testY))\n",
    "\n",
    "        #trainX = trainX [0:5000]\n",
    "        #trainY = trainY [0:5000]\n",
    "        \n",
    "        #num of classes\n",
    "        num_classes=len(np.unique(trainY))\n",
    "        \n",
    "    elif dataset==\"cifar100\":\n",
    "        (trainX, trainY), (testX, testY) = cifar100.load_data()\n",
    " \n",
    "\n",
    "        dim=32\n",
    "    \n",
    "        #concat\n",
    "        trainX = np.concatenate((trainX, testX))\n",
    "        trainY = np.concatenate((trainY, testY))\n",
    "        \n",
    "        #num of classes\n",
    "        num_classes=len(np.unique(trainY))\n",
    "\n",
    "        # reshape dataset to have a single channel\n",
    "        #trainX = trainX.reshape(trainX.shape[0], pix, pix, 3)\n",
    "\n",
    "\n",
    "    elif dataset==\"purchase100\":\n",
    "        df_tot = pd.read_csv(dataset+'_new.csv', header=None)\n",
    "        df_tot.dropna(inplace=True)\n",
    "\n",
    "        trainX = df_tot.iloc[:,1:]\n",
    "        trainY = df_tot.iloc[:,0]\n",
    "               \n",
    "        \n",
    "        dim=trainX.shape[1]\n",
    "\n",
    "        \n",
    "        #num of classes\n",
    "        num_classes=100\n",
    "\n",
    "        trainX=np.array(trainX)\n",
    "        trainY=np.array(trainY)\n",
    "        \n",
    "    elif dataset==\"texas100\":\n",
    "        df_tot_feats = pd.read_csv(dataset+'_feats.csv', header=None)\n",
    "        df_tot_feats.dropna(inplace=True)\n",
    "\n",
    "        df_tot_lab = pd.read_csv(dataset+'_labels.csv', header=None)\n",
    "        df_tot_lab.dropna(inplace=True)\n",
    "        \n",
    "        \n",
    "        trainX = df_tot_feats\n",
    "        trainY = df_tot_lab\n",
    "\n",
    "        dim=trainX.shape[1]\n",
    "\n",
    "       \n",
    "        #num of classes\n",
    "        num_classes=100\n",
    "\n",
    "        trainX=np.array(trainX)\n",
    "        trainY=np.array(trainY) \n",
    "        \n",
    "    elif dataset==\"location\":\n",
    "        df_tot = pd.read_csv(dataset+'.csv', header=None)\n",
    "        df_tot.dropna(inplace=True)\n",
    "\n",
    "     \n",
    "        trainX = df_tot.iloc[:,1:]\n",
    "        trainY = df_tot.iloc[:,0]\n",
    " \n",
    "        dim=trainX.shape[1]\n",
    "\n",
    "       \n",
    "        #num of classes\n",
    "        num_classes=30\n",
    "\n",
    "        trainX=np.array(trainX)\n",
    "        trainY=np.array(trainY) \n",
    "        \n",
    "    elif dataset==\"adult\":\n",
    "        df_tot = pd.read_csv(dataset+'.csv', header=None)\n",
    "        df_tot.dropna(inplace=True)\n",
    "\n",
    "        trainX = df_tot.iloc[:,0:df_tot.shape[1]-1]\n",
    "        trainY = df_tot.iloc[:,-1]\n",
    "\n",
    "        dim=trainX.shape[1]\n",
    "    \n",
    "        #num of classes\n",
    "        num_classes=2\n",
    "    \n",
    "        for j in range(0,trainX.shape[1]):\n",
    "            if trainX.iloc[:,j].dtypes == object:\n",
    "                trainX.iloc[:,j]=trainX.iloc[:,j].astype('category')\n",
    "                trainX.iloc[:,j] = trainX.iloc[:,j].cat.codes\n",
    "            else:\n",
    "                sc = StandardScaler()  \n",
    "                val=np.array(trainX.iloc[:,j]).reshape(-1,1)\n",
    "                std_data = sc.fit_transform(val)\n",
    "                std_data = pd.DataFrame(std_data)\n",
    "                trainX.iloc[:,j]=std_data\n",
    "                \n",
    "        trainY = trainY.astype('category')\n",
    "        trainY = trainY.cat.codes\n",
    " \n",
    "        trainX=np.array(trainX)\n",
    "        trainY=np.array(trainY)\n",
    "          \n",
    "    elif dataset == \"synthetic\":\n",
    "        sz=20000\n",
    "        fin_bin_matrix, bin_class = data_gen(sz)\n",
    "        \n",
    "        trainX = fin_bin_matrix\n",
    "        trainY = bin_class\n",
    " \n",
    "        dim=trainX.shape[1]\n",
    "\n",
    "       \n",
    "        #num of classes\n",
    "        num_classes=2\n",
    "\n",
    "        trainX=np.array(trainX)\n",
    "        trainY=np.array(trainY) \n",
    "        \n",
    " \n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "\n",
    "\n",
    "    return trainX, trainY, num_classes, dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "anticipated-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fin_bin_matrix, bin_class = data_gen(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experimental-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(sz):\n",
    "    size=sz\n",
    "    half_size = int(size/2)\n",
    "    bin_matrix = np.mod(np.random.permutation(size*49).reshape(size,49),1)  \n",
    "    bin_matrix = pd.DataFrame(bin_matrix)\n",
    "    bin_class = np.zeros(size)\n",
    "    bin_class[0:half_size] = 1\n",
    "    for j in range(0,bin_matrix.shape[0]):\n",
    "        if bin_class[j] == 0:\n",
    "            s = np.random.normal(0, 0.01, 49)\n",
    "            bin_matrix.iloc[j] = bin_matrix.iloc[j] + np.abs(s)\n",
    "        else:\n",
    "            s = np.random.normal(0, 0.01, 49)\n",
    "            bin_matrix.iloc[j] = 1 -  np.abs(s)\n",
    "\n",
    "    bin_class = pd.DataFrame(bin_class)\n",
    "    #fin_bin_matrix = pd.concat([bin_matrix,bin_class],axis=1)\n",
    "    return bin_matrix, bin_class\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels(train, test):\n",
    "\t# convert from integers to floats\n",
    "\ttrain_norm = train.astype('float32')\n",
    "\ttest_norm = test.astype('float32')\n",
    "\t# normalize to range 0-1\n",
    "\ttrain_norm = train_norm / 255.0\n",
    "\ttest_norm = test_norm / 255.0\n",
    "\t# return normalized images\n",
    "\treturn train_norm, test_norm\n",
    "\n",
    "\n",
    "# scale pixels\n",
    "def prep_pixels_full(train):\n",
    "    #print(train)\n",
    "    # convert from integers to floats\n",
    "    train_norm = train.astype('float32')\n",
    "    # normalize to range 0-1\n",
    "    train_norm = train_norm / 255.0\n",
    "    # return normalized images\n",
    "    return train_norm\n",
    "\n",
    "\n",
    "\n",
    "def define_attack_model(n_class):\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.add(Activation(\"relu\"))\n",
    "    \n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.add(Activation(\"relu\"))\n",
    "\n",
    "\tmodel.add(Dense(n_class, activation='softmax'))\n",
    "\n",
    "\t# compile model\n",
    "\topt = SGD(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# def extract_activation(model,trainX,trainY,train_ix,act_layer):\n",
    "#     activation_list=[]\n",
    "#     for i, v in enumerate(model.layers):\n",
    "#         print(v)\n",
    "#         s = str(v)\n",
    "#         a=re.split('\\s+', s)\n",
    "#         if a[0]=='<tensorflow.python.keras.layers.core.Activation':\n",
    "#             activation_list.append(i)\n",
    "    \n",
    "#     global_activation = [K.function([model.input], \n",
    "#                               [layer.output])([np.array(trainX), 1])\n",
    "#                               for layer in model.layers]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     global_activation = pd.DataFrame(global_activation[act_layer][0])\n",
    "#     global_activation = global_activation.set_index(train_ix)\n",
    "\n",
    "#     ####################################################################################################  \n",
    "#     dist_data=[]\n",
    "#     pred_mem=[]\n",
    "#     outlier=[]\n",
    "\n",
    "#     class_val = np.unique(np.argmax(trainY, axis=1) )\n",
    "#     trainYY = np.argmax(trainY, axis=1) \n",
    "    \n",
    "#     y=pd.DataFrame(trainYY)\n",
    "#     rec=pd.DataFrame(train_ix)\n",
    "#     train_df=pd.concat([rec,y],axis=1)\n",
    "#     train_df.columns=['rec','y']\n",
    "    \n",
    "#     for c_val in class_val:\n",
    "#        print(\"\\n \",c_val)\n",
    "\n",
    "#        filter_rec_all = train_df[(train_df['y'] == c_val)]\n",
    "#        filter_rec_idx = np.array(filter_rec_all['rec'])\n",
    "       \n",
    "#        filter_rec=global_activation.loc[filter_rec_idx]\n",
    "   \n",
    "#        CLASS_IDX.append(filter_rec_idx)\n",
    "       \n",
    "#        m_rec=np.array(np.mean(pd.DataFrame(filter_rec),axis=0))\n",
    "       \n",
    "#        dist=np.linalg.norm(filter_rec - m_rec,axis=1)\n",
    "#        dist_data.append(dist)\n",
    "       \n",
    "#        d_mean=np.mean(dist)\n",
    "       \n",
    "#        d_mean=np.percentile(dist, 95)\n",
    "       \n",
    "       \n",
    "#        d_dist=pd.DataFrame(dist)       \n",
    "#        d_dist = d_dist.set_index(filter_rec_idx)\n",
    "#        d_dist.columns=['dist'] \n",
    "\n",
    "\n",
    "       \n",
    "#        selected_activation = d_dist[(d_dist['dist'] > d_mean)]   \n",
    "#        activation_idx_to_rec_idx=selected_activation.index.values\n",
    "       \n",
    "       \n",
    "#        #############################################\n",
    "#        pred_mem.append(activation_idx_to_rec_idx)\n",
    "#        #############################################\n",
    "       \n",
    "#        n_neighbors=10\n",
    "#        clf = LocalOutlierFactor(n_neighbors=n_neighbors)\n",
    "#        filter_outlier=clf.fit_predict(filter_rec)\n",
    "       \n",
    "#        filter_outlier_df=pd.DataFrame(filter_outlier)\n",
    "#        filter_outlier_df = filter_outlier_df.set_index(filter_rec_idx)\n",
    "#        filter_outlier_df.columns=['status'] \n",
    "\n",
    "       \n",
    "#        filter_outlier_df = filter_outlier_df[(filter_outlier_df['status'] == -1)]   \n",
    "#        outlier_idx_to_rec_idx = filter_outlier_df.index.values\n",
    "       \n",
    "#        #np.array(list(selected_outlier_idx['row']))\n",
    "\n",
    "#        #############################################\n",
    "#        outlier.append(outlier_idx_to_rec_idx)\n",
    "#        #############################################\n",
    "        \n",
    "    \n",
    "#     knn_outlier = np.array([item for sublist in outlier for item in sublist])\n",
    "#     dist_activation = np.array([item for sublist in pred_mem for item in sublist])\n",
    "\n",
    "#     #all_idx=np.array(list(range(0,len(trainY))))\n",
    "    \n",
    "#     no_knn_outlier=(set(train_ix)-set(knn_outlier))\n",
    "#     no_knn_outlier=np.array(list(no_knn_outlier))\n",
    "\n",
    "#     no_dist_activation=(set(train_ix)-set(dist_activation))\n",
    "#     no_dist_activation=np.array(list(no_dist_activation))\n",
    "    \n",
    "#     KNN_OUT.append(knn_outlier)\n",
    "#     KNN_NOOUT.append(no_knn_outlier)\n",
    "#     DIST_OUT.append(dist_activation)\n",
    "#     DIST_NOOUT.append(no_dist_activation)\n",
    "\n",
    "# ####################################################################################################    \n",
    "\n",
    "#     #global_activation[global_activation>0]=1  \n",
    "#     values=[]\n",
    "#     for i in range(0,global_activation.shape[1]):\n",
    "#         row_values=[]\n",
    "#         for j in range(0,global_activation.shape[0]):\n",
    "#             print(i,j)\n",
    "#             val=1 * (global_activation.iloc[j,i] !=0 ) * ( np.max(global_activation.iloc[j]) -  global_activation.iloc[j,i])\n",
    "#             row_values.append(val)\n",
    "#             values.append(row_values)\n",
    "#     act_values=pd.DataFrame(values).T\n",
    "\n",
    "#     print(act_values)\n",
    "\n",
    "#     L_act_pred_mem=[]\n",
    "#     H_act_pred_mem=[]\n",
    "    \n",
    "#     for c_val in class_val:\n",
    "       \n",
    "#        filter_rec_all = train_df[(train_df['y'] == c_val)]\n",
    "#        filter_rec_idx = np.array(filter_rec_all['rec'])\n",
    "#        filter_rec=global_activation.loc[filter_rec_idx]\n",
    " \n",
    "       \n",
    "#        m_rec=np.array(np.mean(pd.DataFrame(filter_rec),axis=0))\n",
    "       \n",
    "#        dist=np.linalg.norm(filter_rec - m_rec,axis=1)\n",
    "       \n",
    "#        d_mean=np.mean(dist)\n",
    "       \n",
    "       \n",
    "#        d_dist=pd.DataFrame(dist)\n",
    "       \n",
    "#        print(d_dist)\n",
    "       \n",
    "#        d_filter_rec_idx=pd.DataFrame(filter_rec_idx)\n",
    "#        fin_df=pd.concat([d_filter_rec_idx,d_dist],axis=1)\n",
    "#        fin_df.columns=['row','dist']\n",
    "       \n",
    "#        selected_activation_idx= fin_df[(fin_df['dist'] <= d_mean)]\n",
    "#        activation_idx_to_rec_idx=np.array(list(selected_activation_idx['row']))\n",
    "#        L_act_pred_mem.append(activation_idx_to_rec_idx)\n",
    "       \n",
    "#        selected_activation_idx= fin_df[(fin_df['dist'] > d_mean)]\n",
    "#        activation_idx_to_rec_idx=np.array(list(selected_activation_idx['row']))\n",
    "#        H_act_pred_mem.append(activation_idx_to_rec_idx)\n",
    "       \n",
    "#     L_act_pred_mem_idx = np.array([item for sublist in L_act_pred_mem for item in sublist])\n",
    "#     H_act_pred_mem_idx = np.array([item for sublist in H_act_pred_mem for item in sublist])\n",
    "    \n",
    "#     L_act_pred_mem_idx = np.sort(L_act_pred_mem_idx)\n",
    "#     H_act_pred_mem_idx = np.sort(H_act_pred_mem_idx)\n",
    "    \n",
    "    \n",
    "#     Low_activation_idx=L_act_pred_mem_idx\n",
    "#     High_activation_idx=H_act_pred_mem_idx\n",
    "\n",
    "\n",
    "#     #print(Low_activation_idx)\n",
    "#     #print(High_activation_idx)\n",
    "\n",
    "#     HIGH_ACTIVATION.append(High_activation_idx)\n",
    "#     LOW_ACTIVATION.append(Low_activation_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_model(trainX, trainY, testX, testY, ood_x_test, ood_y_test, dd_x_test, dd_y_test, dataset, model_type, num_classes, dim, channel, init_weights):\n",
    "    \n",
    "\t\n",
    "    scores, histories = list(), list()\n",
    "    TRAIN=[]\n",
    "    TEST=[]\n",
    "    pred_status=[]\n",
    "    \n",
    "\t# define model\n",
    "    model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "    #model.set_weights(init_weights)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=EPS, batch_size=32, verbose=0)\n",
    "    \n",
    "\t# evaluate model\n",
    "    _, train_acc = model.evaluate(trainX, trainY, verbose=0)\n",
    "    #print(\"Train acc : \", (train_acc * 100.0))\n",
    "    \n",
    "\t# append scores\n",
    "    #scores.append(test_acc)\n",
    "    histories.append(history)\n",
    "\n",
    "    ### train acc classwisw ###\n",
    "    Y_train_pred = model.predict_classes(trainX, verbose=0)\n",
    "    Y_train_true = np.argmax(trainY, axis=1) \n",
    "    tr_matrix = confusion_matrix(Y_train_true, Y_train_pred)\n",
    "    tr=tr_matrix.diagonal()/tr_matrix.sum(axis=1)                \n",
    "    TRAIN.append(tr)\n",
    "    \n",
    "    ### test acc classwisw ###\n",
    "    Y_test_pred = model.predict_classes(testX, verbose=0)\n",
    "    Y_test_true = np.argmax(testY, axis=1) \n",
    "    te_matrix = confusion_matrix(Y_test_true, Y_test_pred)\n",
    "    te=te_matrix.diagonal()/te_matrix.sum(axis=1)          \n",
    "    TEST.append(te)\n",
    "    \n",
    "\n",
    "    ALL_XTRAIN.append(trainX)\n",
    "    ALL_YTRAIN.append(trainY)\n",
    "    ALL_XTEST.append(testX)\n",
    "    ALL_YTEST.append(testY)\n",
    "\n",
    "    train_pred=model.predict(trainX, batch_size=32)\n",
    "    target_train_performance=(train_pred, Y_train_true.astype('int32'))\n",
    "      \n",
    "    test_pred=model.predict(testX, batch_size=32)\n",
    "    target_test_performance=(test_pred, Y_test_true.astype('int32'))\n",
    "    \n",
    "    _, ind_test_acc = model.evaluate(testX, testY, verbose=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    ood_test_pred = model.predict(ood_x_test, batch_size=32)\n",
    "    Y_ood_pred_class = model.predict_classes(ood_x_test, verbose=0)\n",
    "    Y_ood_test_true = np.argmax(ood_y_test, axis=1)\n",
    "    target_ood_performance=(ood_test_pred, Y_ood_test_true.astype('int32'))\n",
    "    \n",
    "    _, ood_test_acc = model.evaluate(ood_x_test, ood_y_test, verbose=0)\n",
    "\n",
    "    \n",
    "    \n",
    "    dd_test_pred=model.predict(dd_x_test, batch_size=32)\n",
    "    Y_dd_pred_class = model.predict_classes(dd_x_test, verbose=0)\n",
    "    Y_dd_test_true = np.argmax(dd_y_test, axis=1)\n",
    "    target_dd_performance=(dd_test_pred, Y_dd_test_true.astype('int32'))\n",
    "    \n",
    "    _, dd_test_acc = model.evaluate(dd_x_test, dd_y_test, verbose=0)     \n",
    "        \n",
    "    #record prediction status\n",
    "    \n",
    "    #train\n",
    "    tr_pred=(Y_train_true==Y_train_pred)\n",
    "    pred_status.append(tr_pred)\n",
    "    \n",
    "    #test\n",
    "    te_pred=(Y_test_true==Y_test_pred)\n",
    "    pred_status.append(te_pred)\n",
    "    \n",
    "    #ood\n",
    "    ood_pred=(Y_ood_test_true==Y_ood_pred_class)\n",
    "    pred_status.append(ood_pred)\n",
    "    \n",
    "    #dd\n",
    "    dd_pred=(Y_dd_test_true==Y_dd_pred_class)\n",
    "    pred_status.append(dd_pred)\n",
    "    \n",
    "    \n",
    "    print(\"Training Acc : \", train_acc, \" IND Test Acc : \", ind_test_acc ,\" OOD Test Acc : \", ood_test_acc, \" DD Test Acc : \", dd_test_acc)\n",
    "    \n",
    "    return     train_acc, ind_test_acc, histories, TRAIN, TEST , target_test_performance, target_train_performance, model, target_ood_performance, target_dd_performance, pred_status\n",
    "\n",
    "\n",
    "# evaluate a model using k-fold cross-validation\n",
    "def evaluate_mod_shadow_model(trainX, trainY, dataset, model_type, num_classes, dim, channel, init_weights):\n",
    "    \n",
    "\t\n",
    "    scores, histories = list(), list()\n",
    "    TRAIN=[]\n",
    "    TEST=[]\n",
    "    pred_status=[]\n",
    "    \n",
    "\t# define model\n",
    "    model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "    #model.set_weights(init_weights)\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(trainX, trainY, epochs=EPS, batch_size=32, verbose=0)\n",
    "    \n",
    "\t# evaluate model\n",
    "    _, train_acc = model.evaluate(trainX, trainY, verbose=0)\n",
    "   \n",
    "    train_pred=model.predict(trainX, batch_size=32)\n",
    "    trainY = np.argmax(trainY,axis=1)\n",
    "    target_train_performance=(train_pred, trainY.astype('int32'))\n",
    "      \n",
    "\n",
    "    \n",
    "    print(\"Training Acc : \", train_acc )\n",
    "    \n",
    "    return target_train_performance, model\n",
    "\n",
    "\n",
    "\n",
    "# utility function for showing images\n",
    "def show_imgs(x_test, decoded_imgs=None, n=10):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(2, n, i+1)\n",
    "        item_idx=list(range(0,x_test.shape[0]))\n",
    "        random.sample(item_idx, n)\n",
    "        plt.imshow(x_test[i].reshape(28,28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def classwise_show_imgs(dataset, x_test,y_test, strname, decoded_imgs=None, n=5):\n",
    "    idx_list=[]\n",
    "    #sample_per_class=5\n",
    "      \n",
    "    class_vals = np.unique(np.argmax(y_test, axis=1) )\n",
    "    \n",
    "    trainYY = np.argmax(y_test, axis=1) \n",
    "    \n",
    "    for c_val in class_vals:\n",
    "        #print(c_val)\n",
    "        filter_rec_idx= np.where(np.array(trainYY)==c_val)\n",
    "        if len(filter_rec_idx[0])==0:\n",
    "            break;\n",
    "        else:\n",
    "            sample_per_class=1\n",
    "            \n",
    "        \n",
    "        selected_idx=random.sample(list(filter_rec_idx[0]), sample_per_class)\n",
    "        idx_list.append(selected_idx)\n",
    "    \n",
    "    idx_list = [item for sublist in idx_list for item in sublist]\n",
    "\n",
    "    n_size=len(idx_list)\n",
    "    \n",
    "    plt.figure(figsize=(30, 5))\n",
    "    \n",
    "    if dataset in ('MNIST','FMNIST'):\n",
    "        pix=28\n",
    "        for i in range(len(idx_list)):\n",
    "            ax = plt.subplot(2, n_size, i+1)\n",
    "            image_id=idx_list[i]\n",
    "            plt.imshow(x_test[image_id].reshape(pix,pix))\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    else:\n",
    "        pix=32\n",
    "        for i in range(len(idx_list)):\n",
    "            ax = plt.subplot(2, n_size, i+1)\n",
    "            image_id=idx_list[i]\n",
    "            plt.imshow(x_test[image_id])\n",
    "            plt.gray()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)        \n",
    "\n",
    "\n",
    "    #plt.savefig(img_path+strname+fname+'.png')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "def shadow_model(x_shadow, y_shadow, dataset, model_type, num_classes, dim, channel):\n",
    "    \n",
    "    \n",
    "    full_sm_train_pred=[]\n",
    "    full_sm_train_class=[]\n",
    "    \n",
    "    full_sm_test_pred=[]\n",
    "    full_sm_test_class=[]\n",
    "    \n",
    "    full_clz_train=[]\n",
    "    full_clz_test=[]\n",
    "    \n",
    "    members=[]\n",
    "    nonmembers=[]\n",
    "    \n",
    "\n",
    "    for j in range(0,1):\n",
    "        \n",
    "        print(\"Shadow Model \", j)\n",
    "        \n",
    "        x_shadow_train, x_shadow_test, y_shadow_train, y_shadow_test = train_test_split(x_shadow, y_shadow, test_size=0.5)\n",
    "\n",
    "        model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "            \n",
    "        # fit model\n",
    "        history = model.fit(x_shadow_train, y_shadow_train, epochs=EPS, batch_size=32, validation_data=(x_shadow_test, y_shadow_test), verbose=0)\n",
    "    \t\n",
    "        # evaluate model\n",
    "        _, train_acc = model.evaluate(x_shadow_train, y_shadow_train, verbose=0)\n",
    "        _, test_acc = model.evaluate(x_shadow_test, y_shadow_test, verbose=0)\n",
    "        print(\"Shadow Train acc : \", (train_acc * 100.0),\"Shadow Test acc : \", (test_acc * 100.0))\n",
    "    \t\n",
    "    \n",
    "        #train SM\n",
    "        sm_train_pred=model.predict(x_shadow_train, batch_size=32)\n",
    "        sm_train_class=np.argmax(y_shadow_train,axis=1)\n",
    "    \n",
    "    \n",
    "        #test SM\n",
    "        sm_test_pred=model.predict(x_shadow_test, batch_size=32)\n",
    "        sm_test_class=np.argmax(y_shadow_test,axis=1)\n",
    "        \n",
    "     \n",
    "        full_sm_train_pred.append(sm_train_pred)        \n",
    "        full_sm_train_class.append(sm_train_class)\n",
    "        members.append(np.ones(len(sm_train_pred)))\n",
    "        \n",
    "        full_sm_test_pred.append(sm_test_pred)        \n",
    "        full_sm_test_class.append(sm_test_class) \n",
    "        nonmembers.append(np.zeros(len(sm_test_pred)))\n",
    "\n",
    "\n",
    "    full_sm_train_pred = np.vstack(full_sm_train_pred)\n",
    "    full_sm_train_class = [item for sublist in full_sm_train_class for item in sublist]\n",
    "    members = [item for sublist in members for item in sublist]\n",
    "    \n",
    "    full_sm_test_pred = np.vstack(full_sm_test_pred)\n",
    "    full_sm_test_class = [item for sublist in full_sm_test_class for item in sublist]\n",
    "    nonmembers = [item for sublist in nonmembers for item in sublist]\n",
    "    \n",
    "\n",
    "    \n",
    "    shadow_train_performance=(full_sm_train_pred, np.array(full_sm_train_class))\n",
    "    shadow_test_performance=(full_sm_test_pred, np.array(full_sm_test_class))\n",
    "\n",
    "\n",
    "    ###atack data preparation\n",
    "    attack_x = (full_sm_train_pred,full_sm_test_pred)\n",
    "    #attack_x = np.vstack(attack_x)\n",
    "    \n",
    "    attack_y = (np.array(members).astype('int32'),np.array(nonmembers).astype('int32'))\n",
    "    #attack_y = np.concatenate(attack_y)\n",
    "    #attack_y = attack_y.astype('int32')\n",
    "    \n",
    "    \n",
    "    classes = (np.array(full_sm_train_class),np.array(full_sm_test_class))\n",
    "    #classes = np.array([item for sublist in classes for item in sublist])\n",
    "\n",
    "\n",
    "    attack_dataset = (attack_x,attack_y,classes)\n",
    "\n",
    "            \n",
    "    return  shadow_train_performance, shadow_test_performance, attack_dataset, x_shadow_train, y_shadow_train, x_shadow_test, y_shadow_test, model\n",
    "\n",
    "\n",
    "\n",
    "def prety_print_result(mem, pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(mem, pred).ravel()\n",
    "    print('TP: %d     FP: %d     FN: %d     TN: %d' % (tp, fp, fn, tn))\n",
    "    if tp == fp == 0:\n",
    "    \tprint('PPV: 0\\nAdvantage: 0')\n",
    "    else:\n",
    "    \tprint('PPV: %.4f\\nAdvantage: %.4f' % (tp / (tp + fp), tp / (tp + fn) - fp / (tn + fp)))\n",
    "\n",
    "    return tp, fp, fn, tn, (tp / (tp + fp)), (tp / (tp + fn) - fp / (tn + fp)), ((tp+tn)/(tp+tn+fp+fn)),  (tp / (tp + fn))\n",
    "\n",
    "def train_attack_model(attack_data, check_membership, n_hidden=50, learning_rate=0.01, batch_size=200, epochs=50, model='nn', l2_ratio=1e-7):\n",
    "\n",
    "    x, y,  classes = attack_data\n",
    "\n",
    "    train_x = x[0]\n",
    "    train_y = y[0]\n",
    "    test_x = x[1]\n",
    "    test_y = y[1]\n",
    "    train_classes = classes[0]\n",
    "    test_classes = classes[1]\n",
    "    \n",
    "    \n",
    "    checkmem_prediction_vals, checkmem_membership_status, checkmem_class_status = check_membership\n",
    "    \n",
    "    checkmem_prediction_vals=np.vstack(checkmem_prediction_vals)\n",
    "    checkmem_membership_status=np.array([item for sublist in checkmem_membership_status for item in sublist])\n",
    "    checkmem_class_status=np.array([item for sublist in checkmem_class_status for item in sublist])\n",
    "    \n",
    "    train_indices = np.arange(len(train_x))\n",
    "    test_indices = np.arange(len(test_x))\n",
    "    unique_classes = np.unique(train_classes)\n",
    "\n",
    "\n",
    "    predicted_membership, target_membership = [], []\n",
    "    for c in unique_classes:\n",
    "        print(\"Class : \", c)\n",
    "        c_train_indices = train_indices[train_classes == c]\n",
    "        c_train_x, c_train_y = train_x[c_train_indices], train_y[c_train_indices]\n",
    "        c_test_indices = test_indices[test_classes == c]\n",
    "        c_test_x, c_test_y = test_x[c_test_indices], test_y[c_test_indices]\n",
    "        c_dataset = (c_train_x, c_train_y, c_test_x, c_test_y)        \n",
    "        \n",
    "        full_cx_data=(c_train_x,c_test_x)\n",
    "        full_cx_data = np.vstack(full_cx_data)\n",
    "\n",
    "        full_cy_data=(c_train_y,c_test_y)\n",
    "        full_cy_data = np.array([item for sublist in full_cy_data for item in sublist])\n",
    "\n",
    "                \n",
    "        \n",
    "        classifier = define_attack_model(2)\n",
    "        history = classifier.fit(full_cx_data, full_cy_data, epochs=EPS, batch_size=32, verbose=0)\n",
    "\n",
    "        #get predictions on real train and test data\n",
    "        c_indices = np.where(checkmem_class_status==c)\n",
    "        c_pred_y = classifier.predict_classes(checkmem_prediction_vals[c_indices])\n",
    "        c_target_y = checkmem_membership_status[c_indices]\n",
    "        \n",
    "       \n",
    "        target_membership.append(c_target_y)\n",
    "        predicted_membership.append(c_pred_y)\n",
    "\n",
    "    target_membership=np.array([item for sublist in target_membership for item in sublist])\n",
    "    predicted_membership=np.array([item for sublist in predicted_membership for item in sublist])\n",
    "\n",
    "\n",
    "        \n",
    "    tp, fp, fn, tn, precision, advj, acc, recall = prety_print_result (target_membership,predicted_membership)   \n",
    "    return tp, fp, fn, tn, precision, advj, acc, recall\n",
    "\n",
    "\n",
    "    \n",
    "def data_extraction(ind_train_performance,ind_test_performance):\n",
    "\n",
    "    #test size\n",
    "    s_test_pred=ind_test_performance[0].shape[0]\n",
    "    \n",
    "    #train size\n",
    "    s_train_pred=ind_train_performance[0].shape[0]\n",
    "   \n",
    "    \n",
    "    #test size <= train size\n",
    "    if s_test_pred <= s_train_pred:\n",
    "        \n",
    "        #randomly pick indices\n",
    "        indices=list(range(0,s_train_pred))\n",
    "        rec_idx=random.sample(indices, k = s_test_pred)\n",
    "        \n",
    "        trax=ind_train_performance[0][rec_idx]\n",
    "        tray=ind_train_performance[1][rec_idx]\n",
    "\n",
    "        tex=ind_test_performance[0]\n",
    "        tey=ind_test_performance[1]\n",
    " \n",
    "        \n",
    "    #train size <= test size\n",
    "    else:\n",
    "        \n",
    "        #randomly pick indices\n",
    "        indices=list(range(0,s_test_pred))\n",
    "        rec_idx=random.sample(indices, k = s_train_pred)\n",
    " \n",
    "        tex=ind_test_performance[0][rec_idx]\n",
    "        tey=ind_test_performance[1][rec_idx]\n",
    "\n",
    "        trax=ind_train_performance[0]\n",
    "        tray=ind_train_performance[1]\n",
    " \n",
    "    \n",
    "    test_performance = (tex,tey)\n",
    "    train_performance = (trax, tray)       \n",
    "    \n",
    "    return  train_performance, test_performance \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def indices_extraction(ind_train_performance,ind_test_performance):\n",
    "\n",
    "    #test size\n",
    "    s_test_pred=ind_test_performance[0].shape[0]\n",
    "    \n",
    "    #train size\n",
    "    s_train_pred=ind_train_performance[0].shape[0]\n",
    "   \n",
    "    \n",
    "    #test size <= train size\n",
    "    if s_test_pred <= s_train_pred:\n",
    "        \n",
    "        #randomly pick indices\n",
    "        indices=list(range(0,s_train_pred))\n",
    "        rec_idx=random.sample(indices, k = s_test_pred)\n",
    "        \n",
    "        train_indices = rec_idx\n",
    "        test_indices = 0\n",
    "    \n",
    "    #train size <= test size\n",
    "    else:\n",
    "        \n",
    "        #randomly pick indices\n",
    "        indices=list(range(0,s_test_pred))\n",
    "        rec_idx=random.sample(indices, k = s_train_pred)\n",
    " \n",
    "        train_indices = 0\n",
    "        test_indices = rec_idx\n",
    "        \n",
    "    return  train_indices, test_indices \n",
    "\n",
    "\n",
    "\n",
    "def _masked_avg(x, mask, axis=0, esp=1e-10):\n",
    "    return (np.sum(x * mask, axis=axis) / np.maximum(np.sum(mask, axis=axis), esp)).astype(np.float32)\n",
    "def _masked_dot(x, mask, esp=1e-10):\n",
    "    x = x.T.astype(np.float32)\n",
    "    return (np.matmul(x, mask) / np.maximum(np.sum(mask, axis=0, keepdims=True), esp)).astype(np.float32)\n",
    "\n",
    "\n",
    "def intersection(lst1, lst2):\n",
    "    lst3 = [value for value in lst1 if value in lst2]\n",
    "    return lst3\n",
    "   \n",
    "def memorizable_data(trainX, trainY, b_idx, full_df_size, num_rounds,trainX_full, trainY_full):\n",
    "#High_memorizable_data, Low_memorizable_data=memorizable_data(trainX, trainY, b_idx, full_df_size,num_rounds,trainX_full, trainY_full)\n",
    "\n",
    "    import copy\n",
    "    \n",
    "    idx_fin_mem=[]\n",
    "    idx_fin_nmem=[]\n",
    "    Train_Acc=[]\n",
    "    Test_Acc=[]\n",
    "    total_correctness=[]\n",
    "    Fin_rec_idx=[]\n",
    "    trainset_mask=[]\n",
    "\n",
    "    tm_x = copy.deepcopy(trainX)\n",
    "    tm_y = copy.deepcopy(trainY)\n",
    "    \n",
    "    for r in range(1,num_rounds):\n",
    "       \n",
    "        print(\"Run \",r)\n",
    "                \n",
    "        df_tr_x = tm_x\n",
    "        df_tr_y = tm_y\n",
    "\n",
    "        indices = np.array(list(b_idx))\n",
    "        \n",
    "        df_tr_x, df_te_x, df_tr_y, df_te_y,  idx1, idx2  = train_test_split(df_tr_x, df_tr_y, indices, test_size=0.2)\n",
    "                  \n",
    "        \n",
    "        member_index = idx1\n",
    "        non_member_index = idx2\n",
    "    \n",
    "        idx_fin_mem.append(np.array(list(member_index)))\n",
    "        idx_fin_nmem.append(np.array(list(non_member_index)) )               \n",
    "    \n",
    "        \n",
    "        #members vs nonmember\n",
    "        subset_idx=idx1\n",
    "        trainset_mask_t = np.zeros(full_df_size, dtype=np.bool)\n",
    "        trainset_mask_t[subset_idx] = True\n",
    "        Fin_rec_idx.append(subset_idx)\n",
    "            \n",
    "        \n",
    "        y_ori=np.argmax(df_tr_y,axis=1)\n",
    "        test_y_ori=np.argmax(df_te_y,axis=1)\n",
    "        tot_y_ori=np.argmax(trainY_full,axis=1)\n",
    "     \n",
    "\n",
    "        model = define_model(len(np.unique(y_ori)))\n",
    "\t\n",
    "    \n",
    "        # fit model\n",
    "        history = model.fit(df_tr_x, df_tr_y, epochs=EPS, batch_size=32, verbose=0)\n",
    "        \n",
    "        pred = model.predict_classes(trainX_full, verbose=0)\n",
    "        \n",
    "        class_status=pred==np.array(tot_y_ori)\n",
    "        total_correctness.append(class_status)\n",
    "        trainset_mask.append(trainset_mask_t)\n",
    "\n",
    " \n",
    "    trainset_maskx = np.vstack(trainset_mask)\n",
    "    inv_maskx = np.logical_not(trainset_mask)\n",
    "    total_correctness = np.vstack(total_correctness)\n",
    "    #testset_correctnessx = np.vstack(testset_correctness)\n",
    "                    \n",
    "    mem_est = _masked_avg(total_correctness, trainset_maskx) - _masked_avg(total_correctness, inv_maskx)\n",
    "    #infl_est = _masked_dot(testset_correctnessx, trainset_maskx) - _masked_dot(testset_correctnessx, inv_maskx)\n",
    "    \n",
    "    \n",
    "    p = np.percentile(mem_est, 75)\n",
    "    \n",
    "    High_memorizable_data=np.where(mem_est>=0.25)\n",
    "    Low_memorizable_data=np.where(mem_est<0.25)\n",
    "    \n",
    "    return High_memorizable_data, Low_memorizable_data,mem_est\n",
    "\n",
    "def log_loss(a, b):\n",
    "    SMALL_VALUE = 1e-6\n",
    "    return [-np.log(max(b[i,a[i]], SMALL_VALUE)) for i in range(len(a))]\n",
    "\n",
    "\n",
    "def find_ID_and_OOD_data(trainX, trainY, dataset, model_type, num_classes, dim, channel):\n",
    "    \n",
    "    init_model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "    history = init_model.fit(trainX, trainY, epochs=EPS, batch_size=10, verbose=0)\n",
    "        \n",
    "    scores_train_loss = init_model.evaluate(trainX, trainY)\n",
    "\n",
    "    print(init_model.layers[act_layer])\n",
    "        \n",
    "    #train_activations = [K.function([init_model.input], \n",
    "    #                          [layer.output])([np.array(trainX), 1])\n",
    "    #                          for layer in init_model.layers]\n",
    "\n",
    "\n",
    "    layer_output = K.function([init_model.layers[0].input],\n",
    "                                  [init_model.layers[act_layer].output])\n",
    "    \n",
    "    train_activations = layer_output([trainX])[0]\n",
    "\n",
    "    train_activations= pd.DataFrame(train_activations)\n",
    "    \n",
    "    #print(train_activations)\n",
    "    \n",
    "    #train_activations= pd.DataFrame(train_activations[act_layer][0])\n",
    "    y_ori=pd.DataFrame(np.argmax(trainY,axis=1))\n",
    "    y_ori.columns=['y']      \n",
    "    train_df=pd.concat([train_activations,y_ori],axis=1)\n",
    "    \n",
    "    outliers, inliers=[],[]\n",
    "    class_val=list(np.unique(np.array(train_df['y'])))\n",
    "    \n",
    "    \n",
    "    for c_val in class_val:\n",
    "    \n",
    "        filter_rec_all = train_df[(train_df['y'] == c_val)]\n",
    "        filter_rec_idx = np.array(filter_rec_all.index)\n",
    "        \n",
    "        clf = LocalOutlierFactor(n_neighbors=5)\n",
    "        filter_outlier=clf.fit_predict(train_activations.iloc[filter_rec_idx])\n",
    "\n",
    "        #clf = IsolationForest(n_estimators=10)\n",
    "        #filter_outlier = clf.fit(train_activations.iloc[filter_rec_idx])\n",
    "        #filter_outlier    = clf.predict(train_activations.iloc[filter_rec_idx])\n",
    "\n",
    "        toutliers=np.where(filter_outlier==-1)\n",
    "        toutliers=filter_rec_idx[toutliers]\n",
    "        \n",
    "        tinliers=np.where(filter_outlier==1)\n",
    "        tinliers=filter_rec_idx[tinliers]\n",
    "        \n",
    "        outliers.append(toutliers)\n",
    "        inliers.append(tinliers)\n",
    "         \n",
    "        \n",
    "    outliers = [item for sublist in outliers for item in sublist]\n",
    "    inliers = [item for sublist in inliers for item in sublist]\n",
    "\n",
    "    #outliers=random.sample(outliers,len(outliers))\n",
    "    #inliers=random.sample(inliers,len(inliers))\n",
    "    \n",
    "    return outliers, inliers, train_activations\n",
    "\n",
    "\n",
    "def extract_prob(train_prob,test_prob):\n",
    "    train_pred_list=[]\n",
    "    for ins in range(0, train_prob.shape[0]):\n",
    "        val= np.argmax(train_prob[ins])\n",
    "        train_pred_list.append(train_prob[ins][val])\n",
    "    train_pred_list=np.array(train_pred_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_pred_list=[]\n",
    "    for ins in range(0, test_prob.shape[0]):\n",
    "        val= np.argmax(test_prob[ins])\n",
    "        test_pred_list.append(test_prob[ins][val])\n",
    "    \n",
    "    test_pred_list=np.array(test_pred_list)\n",
    "    \n",
    "    return train_pred_list, test_pred_list\n",
    "\n",
    "\n",
    "def extract_datasets(exp, TRZ, TEZ, SHZ, outliers, inliers):\n",
    "    #train test shadow  | WO-with outliers NO-no outliers\n",
    "    print(\"EXP : \", exp)\n",
    "    if exp==1:\n",
    "        print(\"------------NO NO NO------------\")\n",
    "        \n",
    "        train=random.sample(set(inliers), TRZ)\n",
    "        remain_i=set(inliers)-set(train)\n",
    "\n",
    "        test=random.sample(set(remain_i), TEZ)\n",
    "        remain_ii=set(remain_i)-set(test)\n",
    "\n",
    "        shadow=random.sample(set(remain_ii), SHZ)\n",
    "        \n",
    "    elif exp==2:\n",
    "        print(\"------------NO WO NO------------\")\n",
    "        \n",
    "        train=random.sample(set(inliers), TRZ)\n",
    "        remain_i=set(inliers)-set(train)\n",
    "\n",
    "        shadow=random.sample(set(remain_i), SHZ)\n",
    "\n",
    "        test=random.sample(set(outliers), TEZ)\n",
    "        \n",
    "\n",
    "    elif exp==3:\n",
    "        print(\"------------NS NS NS------------\")\n",
    "        all_data=np.concatenate((outliers,inliers))\n",
    "        \n",
    "        train=random.sample(set(all_data), TRZ)\n",
    "        remain_i=set(all_data)-set(train)\n",
    "\n",
    "        test=random.sample(set(remain_i), TEZ)\n",
    "        remain_i=set(remain_i)-set(test)\n",
    "        \n",
    "        shadow=random.sample(set(remain_i), SHZ)\n",
    "        \n",
    "    return train, test, shadow    \n",
    "\n",
    "def sample_selection(trainX_full,trainY_full,k_val,tr_idx):\n",
    "   \n",
    "    class_val = np.unique(np.argmax(trainY_full, axis=1) )\n",
    "\n",
    "    idxY = pd.DataFrame(tr_idx)\n",
    "    trainY = pd.DataFrame(np.argmax(trainY_full, axis=1) )\n",
    "    trainYY=pd.concat([idxY,trainY],axis=1)\n",
    "    trainYY.columns=['idx','y']\n",
    "        \n",
    "    \n",
    "    balance_filter_rec_idx_list=[]\n",
    "    \n",
    "\n",
    "    for c_val in class_val:\n",
    "        print(c_val)\n",
    "\n",
    "        filter_rec_idx = trainYY[(trainYY['y'] == c_val)]\n",
    "        filter_rec_idx =filter_rec_idx['idx']\n",
    "       \n",
    "        \n",
    "        filter_rec_idx=random.sample(list(filter_rec_idx), k = k_val)       \n",
    "        balance_filter_rec_idx_list.append(filter_rec_idx)\n",
    "        \n",
    "    return balance_filter_rec_idx_list\n",
    "\n",
    "#import cv2\n",
    "#import numpy as np\n",
    "\n",
    "def dd_noisy_img_creation(dd_X):\n",
    "    dd_img=[]\n",
    "    for j in dd_X:\n",
    "        img =j\n",
    "        gauss = np.random.normal(0,3,img.size)\n",
    "        gauss = gauss.reshape(img.shape[0],img.shape[1],img.shape[2]).astype('uint8')\n",
    "        noise_img = img + img * gauss\n",
    "        dd_img.append(noise_img)\n",
    "    return dd_img\n",
    "\n",
    "def dd_noisy_img_creation_28pix(dd_X):\n",
    "    dd_img=[]\n",
    "    for j in dd_X:\n",
    "        img =j\n",
    "        #gauss = np.random.normal(8,1,img.size)\n",
    "        gauss = np.random.randn(8,1,img.size)\n",
    "        gauss = gauss.reshape(img.shape[0],img.shape[1],1).astype('uint8')\n",
    "        noise_img = img + img * gauss\n",
    "        dd_img.append(noise_img)\n",
    "    return dd_img\n",
    "\n",
    "\n",
    "def add_salt_pepper_noise (image):\n",
    "  prob = 0.2  \n",
    "  output=np.zeros (image.shape, np.uint8)\n",
    "  thres=1-prob\n",
    "  for i in range (image.shape [0]):\n",
    "    for j in range (image.shape [1]):\n",
    "      rdn=random.random ()\n",
    "      if rdn<prob:\n",
    "        output [i] [j]=0\n",
    "      elif rdn>thres:\n",
    "        output [i] [j]=255\n",
    "      else:\n",
    "        output [i] [j]=image [i] [j]\n",
    "  return output\n",
    "  \n",
    "\n",
    "def dd_noisy_img_creation_sandp(dd_X):\n",
    "    dd_img=[]\n",
    "    for j in dd_X:\n",
    "        img =j\n",
    "        salt_pepper_noise_imgs = add_salt_pepper_noise(img)\n",
    "        dd_img.append(salt_pepper_noise_imgs)\n",
    "    return dd_img\n",
    "\n",
    "def dd_noisy_bin_data_creation(dd_X):\n",
    "    dd_bin=[]\n",
    "    for j in range(0,dd_X.shape[0]):\n",
    "        \n",
    "        selected_idx = random.sample(list(range(0,dd_X.shape[1])), int( dd_X.shape[1]*0.8 ) )\n",
    "        #print(selected_idx)\n",
    "        for i in selected_idx:\n",
    "            #print(i)\n",
    "            #print(j[i])\n",
    "            if dd_X[j,i] == 0 :\n",
    "                dd_X[j,i] = 1 #np.random.choice(5, 1)[0]#1\n",
    "            else:\n",
    "                dd_X[j,i] = 0\n",
    "\n",
    "    return dd_X\n",
    "\n",
    "def dd_noisy_bin_data_binnoise(dd_X,noise=0.9):\n",
    "    dd_bin=[]\n",
    "    for j in range(0,dd_X.shape[0]):\n",
    "        dd_bin.append(np.random.binomial(1,noise, dd_X.shape[1]))\n",
    "    return np.array(dd_bin)\n",
    "\n",
    "def dd_noisy_bin_data(dd_X,noise):\n",
    "    dd_bin=[]\n",
    "    p=noise\n",
    "    dd_X_tmp = copy.deepcopy(dd_X) \n",
    "    for j in range(0,dd_X_tmp.shape[0]):\n",
    "        tmp_dd_X_tmp=dd_X_tmp[j]\n",
    "        for i, value in enumerate(tmp_dd_X_tmp):\n",
    "            v=np.random.choice([value,np.abs(value-1)], p=[1-p,p])\n",
    "            dd_X_tmp[j][i] = 5\n",
    "    return dd_X_tmp\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def fake_members(true_x, true_y, pred_y, classifier, max_t):\n",
    "    true_y_t = np.argmax(true_y,axis=1)\n",
    "    per_instance_loss = np.array(log_loss(true_y_t,pred_y))\n",
    "    #print (per_instance_loss)\n",
    "    \n",
    "    counts = np.zeros(len(true_x))\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        noisy_x = true_x + generate_noise(1)\n",
    "                    \n",
    "        pred_y = classifier.predict(noisy_x, batch_size=32)\n",
    "\n",
    "        noisy_per_instance_loss = np.array(log_loss(true_y_t, pred_y))\n",
    "        \n",
    "        counts += np.where(noisy_per_instance_loss > per_instance_loss, 1, 0)\n",
    "    return counts, per_instance_loss\n",
    "\n",
    "\n",
    "def loss_thre_setting(tr_values, te_values, train_class, test_class):\n",
    "    value_list = np.concatenate((tr_values, te_values))\n",
    "    class_vals = np.unique(np.append(train_class, test_class))\n",
    "    thresh = dict()\n",
    "    #print(class_vals)\n",
    "    for clz in class_vals:\n",
    "        print(clz)\n",
    "        train_ind = np.where(train_class == clz)\n",
    "        test_ind = np.where(test_class == clz)\n",
    "        #value_sd = np.std(value_list)\n",
    "        thre, max_acc = 0, 0\n",
    "        value_list_tmp = np.concatenate((tr_values[train_ind], te_values[test_ind]))\n",
    "        for value in value_list_tmp:\n",
    "            tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "            te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "            acc = 0.5*(tr_ratio + te_ratio)\n",
    "            if acc > max_acc:\n",
    "                thre, max_acc = value, acc\n",
    "        thresh[clz] = thre\n",
    "    return thresh   \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bigger-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shadow_model_attack(x_shadow, y_shadow, dataset, model_type, num_classes, dim, channel):\n",
    "    \n",
    "    \n",
    "    full_sm_train_pred=[]\n",
    "    full_sm_train_class=[]\n",
    "    \n",
    "    full_sm_test_pred=[]\n",
    "    full_sm_test_class=[]\n",
    "    \n",
    "    full_clz_train=[]\n",
    "    full_clz_test=[]\n",
    "    \n",
    "    members=[]\n",
    "    nonmembers=[]\n",
    "    \n",
    "\n",
    "    for j in range(0,1):\n",
    "        \n",
    "        print(\"Shadow Model \", j)\n",
    "        \n",
    "        x_shadow_train, x_shadow_test, y_shadow_train, y_shadow_test = train_test_split(x_shadow, y_shadow, test_size=0.5)\n",
    "\n",
    "        model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "            \n",
    "        # fit model\n",
    "        history = model.fit(x_shadow_train, y_shadow_train, epochs=EPS, batch_size=32, validation_data=(x_shadow_test, y_shadow_test), verbose=0)\n",
    "    \t\n",
    "        # evaluate model\n",
    "        _, train_acc = model.evaluate(x_shadow_train, y_shadow_train, verbose=0)\n",
    "        _, test_acc = model.evaluate(x_shadow_test, y_shadow_test, verbose=0)\n",
    "        print(\"Shadow Train acc : \", (train_acc * 100.0),\"Shadow Test acc : \", (test_acc * 100.0))\n",
    "    \t\n",
    "    \n",
    "        #train SM\n",
    "        sm_train_pred=model.predict(x_shadow_train, batch_size=32)\n",
    "        sm_train_class=np.argmax(y_shadow_train,axis=1)\n",
    "    \n",
    "    \n",
    "        #test SM\n",
    "        sm_test_pred=model.predict(x_shadow_test, batch_size=32)\n",
    "        sm_test_class=np.argmax(y_shadow_test,axis=1)\n",
    "        \n",
    "     \n",
    "        full_sm_train_pred.append(sm_train_pred)        \n",
    "        full_sm_train_class.append(sm_train_class)\n",
    "        members.append(np.ones(len(sm_train_pred)))\n",
    "        \n",
    "        full_sm_test_pred.append(sm_test_pred)        \n",
    "        full_sm_test_class.append(sm_test_class) \n",
    "        nonmembers.append(np.zeros(len(sm_test_pred)))\n",
    "\n",
    "\n",
    "    full_sm_train_pred = np.vstack(full_sm_train_pred)\n",
    "    full_sm_train_class = [item for sublist in full_sm_train_class for item in sublist]\n",
    "    members = [item for sublist in members for item in sublist]\n",
    "    \n",
    "    full_sm_test_pred = np.vstack(full_sm_test_pred)\n",
    "    full_sm_test_class = [item for sublist in full_sm_test_class for item in sublist]\n",
    "    nonmembers = [item for sublist in nonmembers for item in sublist]\n",
    "    \n",
    "\n",
    "    \n",
    "    shadow_train_performance=(full_sm_train_pred, np.array(full_sm_train_class))\n",
    "    shadow_test_performance=(full_sm_test_pred, np.array(full_sm_test_class))\n",
    "\n",
    "\n",
    "    ###atack data preparation\n",
    "    attack_x = (full_sm_train_pred,full_sm_test_pred)\n",
    "    #attack_x = np.vstack(attack_x)\n",
    "    \n",
    "    attack_y = (np.array(members).astype('int32'),np.array(nonmembers).astype('int32'))\n",
    "    #attack_y = np.concatenate(attack_y)\n",
    "    #attack_y = attack_y.astype('int32')\n",
    "    \n",
    "    \n",
    "    classes = (np.array(full_sm_train_class),np.array(full_sm_test_class))\n",
    "    #classes = np.array([item for sublist in classes for item in sublist])\n",
    "\n",
    "\n",
    "    attack_dataset = (attack_x,attack_y,classes)\n",
    "\n",
    "            \n",
    "    return  shadow_train_performance, shadow_test_performance, attack_dataset, x_shadow_train, y_shadow_train, x_shadow_test, y_shadow_test, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complex-division",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shadow_model_noisy (x_shadow, y_shadow, dataset, model_type, num_classes, dim, channel):\n",
    "    \n",
    "    \n",
    "    full_sm_train_pred=[]\n",
    "    full_sm_train_class=[]\n",
    "    \n",
    "    full_sm_test_pred=[]\n",
    "    full_sm_test_class=[]\n",
    "    \n",
    "    full_clz_train=[]\n",
    "    full_clz_test=[]\n",
    "    \n",
    "    members=[]\n",
    "    nonmembers=[]\n",
    "    \n",
    "\n",
    "    for j in range(0,1):\n",
    "        \n",
    "        print(\"Shadow Model \", j)\n",
    "        \n",
    "        x_shadow_train, x_shadow_test, y_shadow_train, y_shadow_test = train_test_split(x_shadow, y_shadow, test_size=0.5)\n",
    "\n",
    "        #get noisy shadow data\n",
    "        if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "\n",
    "            mean = 0.0   # some constant\n",
    "            sd = 0.1    # some constant (standard deviation)\n",
    "            noisy_img = x_shadow_test + np.random.normal(mean, sd, x_shadow_test.shape)\n",
    "            x_shadow_test = np.clip(noisy_img, 0, 255)\n",
    "\n",
    "            #distance to original\n",
    "            #dist_list=np.array(distf(true_x, noisy_x,\"img\"))\n",
    "       \n",
    "            \n",
    "        else:\n",
    "            sd = 0.3\n",
    "            x_shadow_test = dd_noisy_bin_data(x_shadow_test,sd)\n",
    "          \n",
    "            #distance to original\n",
    "            #dist_list=np.array(distf(true_x, noisy_x,\"-\"))\n",
    "        \n",
    "        \n",
    "        model, act_layer = define_model(dataset, model_type, num_classes, dim, channel)\n",
    "            \n",
    "        # fit model\n",
    "        history = model.fit(x_shadow_train, y_shadow_train, epochs=EPS, batch_size=32, validation_data=(x_shadow_test, y_shadow_test), verbose=0)\n",
    "    \t\n",
    "        # evaluate model\n",
    "        _, train_acc = model.evaluate(x_shadow_train, y_shadow_train, verbose=0)\n",
    "        _, test_acc = model.evaluate(x_shadow_test, y_shadow_test, verbose=0)\n",
    "        print(\"Shadow Train acc : \", (train_acc * 100.0),\"Shadow Test acc : \", (test_acc * 100.0))\n",
    "    \t\n",
    "    \n",
    "        #train SM\n",
    "        sm_train_pred=model.predict(x_shadow_train, batch_size=32)\n",
    "        sm_train_class=np.argmax(y_shadow_train,axis=1)\n",
    "    \n",
    "    \n",
    "        #test SM\n",
    "        sm_test_pred=model.predict(x_shadow_test, batch_size=32)\n",
    "        sm_test_class=np.argmax(y_shadow_test,axis=1)\n",
    "        \n",
    "     \n",
    "        full_sm_train_pred.append(sm_train_pred)        \n",
    "        full_sm_train_class.append(sm_train_class)\n",
    "        members.append(np.ones(len(sm_train_pred)))\n",
    "        \n",
    "        full_sm_test_pred.append(sm_test_pred)        \n",
    "        full_sm_test_class.append(sm_test_class) \n",
    "        nonmembers.append(np.zeros(len(sm_test_pred)))\n",
    "\n",
    "\n",
    "    full_sm_train_pred = np.vstack(full_sm_train_pred)\n",
    "    full_sm_train_class = [item for sublist in full_sm_train_class for item in sublist]\n",
    "    members = [item for sublist in members for item in sublist]\n",
    "    \n",
    "    full_sm_test_pred = np.vstack(full_sm_test_pred)\n",
    "    full_sm_test_class = [item for sublist in full_sm_test_class for item in sublist]\n",
    "    nonmembers = [item for sublist in nonmembers for item in sublist]\n",
    "    \n",
    "\n",
    "    \n",
    "    shadow_train_performance=(full_sm_train_pred, np.array(full_sm_train_class))\n",
    "    shadow_test_performance=(full_sm_test_pred, np.array(full_sm_test_class))\n",
    "\n",
    "\n",
    "    ###atack data preparation\n",
    "    attack_x = (full_sm_train_pred,full_sm_test_pred)\n",
    "    #attack_x = np.vstack(attack_x)\n",
    "    \n",
    "    attack_y = (np.array(members).astype('int32'),np.array(nonmembers).astype('int32'))\n",
    "    #attack_y = np.concatenate(attack_y)\n",
    "    #attack_y = attack_y.astype('int32')\n",
    "    \n",
    "    \n",
    "    classes = (np.array(full_sm_train_class),np.array(full_sm_test_class))\n",
    "    #classes = np.array([item for sublist in classes for item in sublist])\n",
    "\n",
    "\n",
    "    attack_dataset = (attack_x,attack_y,classes)\n",
    "\n",
    "            \n",
    "    return  shadow_train_performance, shadow_test_performance, attack_dataset, x_shadow_train, y_shadow_train, x_shadow_test, y_shadow_test, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cooperative-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Merlin attack ##################################\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "def merlin_attack (x, y, x_test, y_test, train_prob, test_prob, model, sd, datatype, max_t):\n",
    "    \n",
    "    precision_merlin, recall_merlin, acc_merlin = [], [], []\n",
    "    \n",
    "    train_counts, train_per_instance_loss, tr_dist_ic_list, tr_status = loss_increase_counts(x, y, train_prob[0], model, sd, datatype, max_t)\n",
    "    test_counts, test_per_instance_loss, te_dist_ic_list, te_status  = loss_increase_counts(x_test, y_test, test_prob[0],model, sd, datatype, max_t)\n",
    "\n",
    "    merlin_thresh = thre_setting(train_counts, test_counts)\n",
    "        \n",
    "    f_train_dist = [item for sublist in  tr_dist_ic_list for item in sublist]\n",
    "    f_test_dist  = [item for sublist in te_dist_ic_list for item in sublist]\n",
    "\n",
    "    ba_thresh = thre_setting(f_train_dist,f_test_dist)\n",
    "\n",
    "    #ba_thresh = mod_thre_setting (f_train_dist,f_test_dist)\n",
    "    \n",
    "    #print(tr_dist_ic_list)\n",
    "    \n",
    "    dist_list= (tr_dist_ic_list,te_dist_ic_list)\n",
    "    \n",
    "    \n",
    "    return merlin_thresh, train_counts, test_counts, ba_thresh, tr_dist_ic_list, te_dist_ic_list, tr_status, te_status  \n",
    "\n",
    "    \n",
    "def merlin_exe (merlin_thresh,train_counts,test_counts):\n",
    "    #IND data \n",
    "    train_merlin_members =  np.where(train_counts >= merlin_thresh, 1, 0)\n",
    "    test_merlin_members =  np.where(test_counts >= merlin_thresh, 1, 0)\n",
    "\n",
    "    #train vs test\n",
    "    tp = np.sum(train_merlin_members==1)\n",
    "    fp = np.sum(test_merlin_members==1)\n",
    "    tn = np.sum(test_merlin_members==0)\n",
    "    fn = np.sum(train_merlin_members==0)\n",
    "\n",
    "    print(\"TP : \",tp)\n",
    "    print(\"TN : \",tn)\n",
    "    print(\"FN : \",fn)\n",
    "    print(\"FP : \",fp)\n",
    "\n",
    "\n",
    "    precision = (tp/(tp+fp))\n",
    "    recall = (tp/(tp+fn))\n",
    "    acc = (tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "    print(\"Precision : \", (tp/(tp+fp)))\n",
    "    print(\"Recall : \", (tp/(tp+fn)))\n",
    "    print(\"Acc : \", acc)\n",
    "    \n",
    "    return precision, recall, acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_loss(a, b):\n",
    "    SMALL_VALUE = 1e-6\n",
    "    return [-np.log(max(b[i,a[i]], SMALL_VALUE)) for i in range(len(a))]\n",
    "\n",
    "def generate_noise(shape,sd):\n",
    "    mu, sigma = 0, sd # mean and standard deviation\n",
    "    \n",
    "    noise = np.random.normal(mu, sigma, shape)\n",
    "   \n",
    "    return noise  \n",
    "    \n",
    "def loss_increase_counts(true_x, true_y, pred_y, classifier, sd, datatype, max_t):\n",
    "    \n",
    "    true_y_t = np.argmax(true_y,axis=1)\n",
    "    \n",
    "    per_instance_loss = np.array(log_loss(true_y_t,pred_y))\n",
    "   \n",
    "    counts = np.zeros(len(true_x))\n",
    "    \n",
    "    mean_dist_cc, mean_dist_ic, dist_ic_list, mem_idx, nm_idx, membership, class_status  = [], [], [], [], [], [], []\n",
    "\n",
    "    ori_y = classifier.predict(true_x, batch_size=32)   \n",
    "    correct_ori_pred = np.where(np.argmax(ori_y,axis=1)==true_y_t)[0]\n",
    "    incorrect_ori_pred = np.where(np.argmax(ori_y,axis=1)!=true_y_t)[0]\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "\n",
    "            mean = 0.0   # some constant\n",
    "            #std = sd    # some constant (standard deviation)\n",
    "            noisy_img = true_x + np.random.normal(mean, sd, true_x.shape)\n",
    "            noisy_x = np.clip(noisy_img, 0, 255)\n",
    "\n",
    "            #distance to original\n",
    "            dist_list=np.array(distf(true_x, noisy_x,\"img\"))\n",
    "       \n",
    "            \n",
    "        else:\n",
    "            noisy_x = dd_noisy_bin_data(true_x,sd)\n",
    "          \n",
    "            #distance to original\n",
    "            dist_list=np.array(distf(true_x, noisy_x,\"-\"))\n",
    "       \n",
    "        #processing for merlin attack\n",
    "        pred_y = classifier.predict(noisy_x, batch_size=32)\n",
    "        \n",
    "        noisy_per_instance_loss = np.array(log_loss(true_y_t, pred_y))\n",
    "        \n",
    "        counts += np.where(noisy_per_instance_loss > per_instance_loss, 1, 0)\n",
    "        \n",
    "        \n",
    "        #processing for boundary attack\n",
    "        \n",
    "        noisy_y = classifier.predict(noisy_x, batch_size=32)   \n",
    "        correct_noisy_idx = np.where(np.argmax(noisy_y,axis=1)==true_y_t)[0]\n",
    "        incorrect_noisy_idx = np.where(np.argmax(noisy_y,axis=1)!=true_y_t)[0]\n",
    "          \n",
    "        \n",
    "        \n",
    "        if datatype == \"shadow\":\n",
    "\n",
    "            if len(incorrect_ori_pred)>0:\n",
    "                dist_list[incorrect_ori_pred]=0    \n",
    "                        \n",
    "            #get only the ones with incorrect classifications - dist should be higher for training low for test        \n",
    "            dist_ic_list.append(dist_list[incorrect_noisy_idx])\n",
    "            #print(\"incorrect_noisy_idx : \", len(incorrect_noisy_idx))\n",
    "            \n",
    " \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            #if len(incorrect_ori_pred)>0:\n",
    "            #    dist_list[incorrect_ori_pred]=0    \n",
    "  \n",
    "            #dist_ic_list.append(dist_list[incorrect_noisy_idx])\n",
    "            #(\"incorrect_noisy_idx : \", len(incorrect_noisy_idx))\n",
    "\n",
    "            dist_ic_list.append(dist_list)  \n",
    "            \n",
    "            \n",
    "            pred_y = classifier.predict(noisy_x, batch_size=32)   \n",
    "            status = np.where(np.argmax(pred_y,axis=1)==true_y_t,1,0)\n",
    "            class_status.append(status)\n",
    " \n",
    "            \n",
    " \n",
    "    return counts, per_instance_loss, dist_ic_list,class_status    \n",
    "    \n",
    "def get_inference_threshold(pred_vector, true_vector, fpr_threshold=None):\n",
    "    fpr, tpr, thresholds = roc_curve(true_vector, pred_vector, pos_label=1)\n",
    "        # return inference threshold corresponding to maximum advantage\n",
    "    if fpr_threshold == None:\n",
    "        return thresholds[np.argmax(tpr-fpr)]\n",
    "        # return inference threshold corresponding to fpr_threshold\n",
    "    for a, b in zip(fpr, thresholds):\n",
    "        if a > fpr_threshold:\n",
    "            break\n",
    "        alpha_thresh = b\n",
    "    return alpha_thresh    \n",
    "\n",
    "\n",
    "def thre_setting(tr_values, te_values):\n",
    "    value_list = np.concatenate((tr_values, te_values))\n",
    "    value_sd = np.std(value_list)\n",
    "    thre, max_acc = 0, 0\n",
    "    for value in value_list:\n",
    "        tr_ratio = np.sum(tr_values>=value)/(len(tr_values)+0.0)\n",
    "        te_ratio = np.sum(te_values<value)/(len(te_values)+0.0)\n",
    "        acc = 0.5*(tr_ratio + te_ratio)\n",
    "        if acc > max_acc:\n",
    "            thre, max_acc = value, acc\n",
    "            #thre=thre+np.choice()\n",
    "    return thre     \n",
    "\n",
    "\n",
    "\n",
    "def mod_thre_setting(f_train_dist,f_test_dist):\n",
    "    \n",
    "    num_increments = 100\n",
    "    \n",
    "    tau_increment = np.amax([np.amax(f_train_dist), np.amax(f_test_dist)]) / num_increments\n",
    "\n",
    "    acc_max = 0.0\n",
    "    \n",
    "    distance_threshold_tau = 0.0\n",
    "    \n",
    "    for i_tau in range(1, num_increments):\n",
    "        is_member_train = np.where(f_train_dist > i_tau * tau_increment, 1, 0)\n",
    "        \n",
    "        is_member_test = np.where(f_test_dist > i_tau * tau_increment, 1, 0)\n",
    "        \n",
    "        \n",
    "        acc = (np.sum(is_member_train) + (is_member_test.shape[0] - np.sum(is_member_test))) / (\n",
    "                is_member_train.shape[0] + is_member_test.shape[0]\n",
    "            )\n",
    "    \n",
    "        if acc > acc_max:\n",
    "            distance_threshold_tau = i_tau * tau_increment\n",
    "            acc_max = acc\n",
    "\n",
    "    return distance_threshold_tau\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def distf(true_x, noisy_x,dt):\n",
    "    dist_arr=[]\n",
    "    for j in range(true_x.shape[0]):\n",
    "        if dt==\"img\":\n",
    "            d = dist.euclidean(true_x[j].flatten(), noisy_x[j].flatten()) \n",
    "        else:\n",
    "            d = np.sum(true_x[j]!=noisy_x[j])/true_x[j].shape[0]\n",
    "                \n",
    "        dist_arr.append(d)\n",
    "    return dist_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sought-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_attack_results (true_membership, pred_membership_ba):\n",
    "    #true_membership = np.concatenate([np.ones(x_train[:num_rec].shape[0]), np.zeros(x_test[:num_rec].shape[0])])\n",
    "    #pred_membership_ba = np.append(np.array(train_membership),np.array(test_membership))\n",
    "\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership_ba).ravel()\n",
    "\n",
    "    print(\"TP : \", tp )\n",
    "    print(\"TN : \", tn)\n",
    "    print(\"FN : \", fn )\n",
    "    print(\"FP : \", fp,\"\\n\")                                  \n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "    print(\"precision : \",precision)\n",
    "    print(\"recall : \",recall)\n",
    "    print(\"acc : \", acc)                                  \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    return precision, recall, acc\n",
    "    \n",
    "def boundary_attack_exe (ba_thresh, dist, status):\n",
    "    \n",
    "    nm_idx, mem_idx =[], [] \n",
    "    \n",
    "    members, nonmembers =  [], []\n",
    "\n",
    "    for i in range(0,len(dist)):\n",
    "        #print(i)\n",
    "        #incorrecly classified, dist higher than t -> members\n",
    "        #print( status[i]==0))\n",
    "        ICC_MEM = np.where( ( status[i]==0) & (dist[i] > ba_thresh) )[0]\n",
    "        members.append(ICC_MEM)\n",
    "        \n",
    "        #incorrecly classified, dist less than t -> nonmembers\n",
    "        ICC_NMEM = np.where( ( status[i]==0) & (dist[i] < ba_thresh) )[0]\n",
    "        nonmembers.append(ICC_NMEM)\n",
    "        \n",
    "        #correcly classified, dist higher than t -> members\n",
    "        CC_MEM = np.where( ( status[i]==1) & (dist[i] > ba_thresh) )[0]\n",
    "        members.append(CC_MEM)\n",
    "        \n",
    "        #correcly classified, dist less than t -> could  be members or nonmembers -> approx as members\n",
    "        CC_MN = np.where( ( status[i]==1) & (dist[i] < ba_thresh) )[0]        \n",
    "        nonmembers.append(CC_MN)\n",
    "       \n",
    "    nm_flat_list = [item for sublist in nonmembers for item in sublist]\n",
    "    mem_flat_list = [item for sublist in members for item in sublist]\n",
    "    \n",
    "    \n",
    "    nm_counter=Counter(nm_flat_list)\n",
    "    mm_counter=Counter(mem_flat_list)\n",
    "\n",
    "    tmp_key=list(nm_counter.keys()),list(mm_counter.keys())\n",
    "    all_keys = np.unique([item for sublist in tmp_key for item in sublist])\n",
    "    \n",
    "    #all_keys = list(np.unique(nm_counter.keys(),mm_counter.keys())[0][0])        \n",
    "        \n",
    "\n",
    "    membership = []\n",
    "    for key in all_keys:\n",
    "        cnt_nm = nm_counter[key]\n",
    "        cnt_mm = mm_counter[key]\n",
    "\n",
    "        if cnt_nm >= cnt_mm:\n",
    "            membership.append(0)\n",
    "        else:\n",
    "            membership.append(1)\n",
    "                \n",
    "    return np.array(membership)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-montreal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "leading-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mia(loss_tresh_list, shadow_train_instance_loss, shadow_test_instance_loss, train_class,test_class):\n",
    "    all_classes = np.append(train_class,test_class)\n",
    "    unique_classes = np.unique(all_classes)\n",
    "    fn, tp, fp, tn = 0, 0, 0, 0\n",
    "    \n",
    "    loss_outlier=[]\n",
    "    loss_inliers=[]\n",
    "    \n",
    "    \n",
    "    for clz in unique_classes:\n",
    "        train_ind = np.where(train_class == clz)\n",
    "        test_ind = np.where(test_class == clz)\n",
    "        thre = loss_tresh_list[clz]              \n",
    "        \n",
    "        fn += np.sum(shadow_train_instance_loss[train_ind]>=thre)\n",
    "        tp += np.sum(shadow_train_instance_loss[train_ind]<thre)\n",
    "            \n",
    "        fp += np.sum(shadow_test_instance_loss[test_ind]<thre)\n",
    "        tn += np.sum(shadow_test_instance_loss[test_ind]>=thre)\n",
    "        \n",
    "        #loss_inliers.append( np.where( t_tshadow_train_instance_loss[train_ind] >= thre))\n",
    "        #loss_outlier.append( np.where( t_te_values[self.t_te_labels==num] >= thre, 1,0))\n",
    " \n",
    "    \n",
    " \n",
    "    print(\"TP : \", tp )\n",
    "    print(\"TN : \", tn)\n",
    "    print(\"FN : \", fn )\n",
    "    print(\"FP : \", fp)\n",
    "\n",
    "        \n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "        \n",
    "    acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "    print(\"precision : \",precision)\n",
    "    print(\"recall : \",recall)\n",
    "    print(\"acc : \", acc)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    return precision, recall, acc\n",
    "\n",
    "def extract_activation(model,trainX,trainY,act_layer):\n",
    "    activation_list=[]\n",
    " \n",
    "    print(\"Layer list : \")\n",
    "    print(model.layers)\n",
    "    \n",
    "    global_activation = [K.function([model.input], \n",
    "                              [layer.output])([np.array(trainX), 1])\n",
    "                              for layer in model.layers]\n",
    "\n",
    "    \n",
    "    #print(pd.DataFrame(global_activation[act_layer][0]))\n",
    "    #print(pd.DataFrame(global_activation[act_layer]))\n",
    "    \n",
    "    global_Df = pd.DataFrame(global_activation[act_layer][0])\n",
    "    \n",
    "\n",
    "    \n",
    "    train_ix = list(range(trainY.shape[0]))\n",
    "    global_Df['y'] = np.argmax(trainY,axis=1)\n",
    "    class_val = np.unique(np.argmax(trainY,axis=1))\n",
    "    train_df = global_Df\n",
    "    \n",
    "    return train_df\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def activation_mia(act_df_full,class_val,act_layer,nn):\n",
    "    \n",
    "    CLASS_IDX, dist_data, pred_mem, outlier, inlier = [], [], [], [], []\n",
    "\n",
    "    #print(act_df_full.head())\n",
    "    \n",
    "    for c_val in class_val:\n",
    "\n",
    "        filter_rec_all = act_df_full[(act_df_full['y'] == c_val)]\n",
    "        filter_rec_idx = np.array(filter_rec_all.index)\n",
    "\n",
    "        \n",
    "        filter_rec=act_df_full.loc[filter_rec_idx]\n",
    "        filter_rec=filter_rec.loc[:, filter_rec.columns != 'y']\n",
    "\n",
    "        #print(c_val)\n",
    "\n",
    "        clf = LocalOutlierFactor(n_neighbors=nn)\n",
    "        filter_outlier=clf.fit_predict(filter_rec)\n",
    "\n",
    "        filter_outlier_df = pd.DataFrame(filter_outlier)\n",
    "        filter_outlier_df = filter_outlier_df.set_index(filter_rec_idx)\n",
    "        filter_outlier_df.columns=['status'] \n",
    "\n",
    "        \n",
    "        \n",
    "        #print(filter_outlier_df.loc[filter_outlier_df['status'] != -1]  )\n",
    "        #print(filter_outlier_df.loc[filter_outlier_df['status'] == 1]  )\n",
    "        \n",
    "        outlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == -1]\n",
    "        outlier_idx_to_rec_idx = outlier_df.index.values\n",
    "        #print(outlier_idx_to_rec_idx)\n",
    "\n",
    "        inlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 1]    \n",
    "        #print(filter_inlier_df)\n",
    "        inlier_idx_to_rec_idx = inlier_df.index.values\n",
    "        #print(inlier_idx_to_rec_idx)\n",
    "\n",
    "        outlier.append(outlier_idx_to_rec_idx)\n",
    "        inlier.append(inlier_idx_to_rec_idx)\n",
    "        \n",
    "\n",
    "    nonmembers = np.array([item for sublist in outlier for item in sublist])\n",
    "    members = np.array([item for sublist in inlier for item in sublist])\n",
    "        \n",
    "    members=np.array(list(members))\n",
    "    nonmembers=np.array(list(nonmembers))\n",
    "    \n",
    "    #print(\"members : \", inlier)\n",
    "    #print(\"nonmembers : \", len(nonmembers))\n",
    "    \n",
    "    pred_membership = np.concatenate([np.ones(len(members)), np.zeros(len(nonmembers))])\n",
    "    \n",
    "    #print(len(nonmembers))\n",
    "    \n",
    "    return pred_membership\n",
    "    \n",
    "    \n",
    "def prep_attack_data(n_attack_data):\n",
    "\n",
    "    attack_mem = pd.DataFrame(n_attack_data[0][0])\n",
    "    attack_nmem = pd.DataFrame(n_attack_data[0][1])\n",
    "    \n",
    "    attack_mem_status = pd.DataFrame(n_attack_data[1][0])\n",
    "    attack_mem_status.columns = [\"membership\"]\n",
    "    \n",
    "    attack_nmem_status = pd.DataFrame(n_attack_data[1][1])\n",
    "    attack_nmem_status.columns = [\"membership\"]\n",
    "    \n",
    "    real_class_mem = pd.DataFrame(n_attack_data[2][0])\n",
    "    real_class_mem.columns = [\"y\"]\n",
    "    \n",
    "    real_class_nmem = pd.DataFrame(n_attack_data[2][1])\n",
    "    real_class_nmem.columns = [\"y\"]\n",
    "\n",
    "    memdf = pd.concat([attack_mem,attack_nmem],axis=0)\n",
    "    memdf = memdf.reset_index(drop=True)\n",
    "\n",
    "    memstatus =  pd.concat([attack_mem_status,attack_nmem_status],axis=0)\n",
    "    memstatus = memstatus.reset_index(drop=True)\n",
    "\n",
    "    realclass = pd.concat([real_class_mem,real_class_nmem],axis=0)\n",
    "    realclass = realclass.reset_index(drop=True)\n",
    "\n",
    "    attack_df = pd.concat([memdf,realclass,memstatus],axis=1)\n",
    "    \n",
    "    return attack_df\n",
    "\n",
    "\n",
    "def shokri_attack(attack_df, mem_validation, nmem_validation):\n",
    "    \n",
    "    predicted_membership, predicted_nmembership, true_membership, TP_idx, TN_idx  = [], [], [], [], []\n",
    "\n",
    "    class_val = np.unique(attack_df['y'])\n",
    "    ncval=attack_df.shape[1]-1\n",
    "    \n",
    "    for c_val in class_val:\n",
    "\n",
    "        print(c_val)\n",
    "        \n",
    "        filter_rec_all = attack_df[(attack_df['y'] == c_val)]\n",
    "        filter_rec_idx = np.array(filter_rec_all.index)\n",
    "        \n",
    "        attack_feat = filter_rec_all.iloc[:, 0:ncval]\n",
    "        attack_class = filter_rec_all['membership']\n",
    "             \n",
    "        d=1\n",
    "        pix = attack_feat.shape[1]\n",
    "        \n",
    "        attack_model, _ = attack_mlp(pix,d)\n",
    "        \n",
    "       \n",
    "        history = attack_model.fit(attack_feat, attack_class, epochs=EPS, batch_size=32, verbose=0)\n",
    "        \n",
    "        mcval=mem_validation.shape[1]-1\n",
    "        \n",
    "        \n",
    "        check_mem_feat = mem_validation[mem_validation['y']==c_val]\n",
    "        check_nmem_feat = nmem_validation[nmem_validation['y']==c_val]\n",
    "        \n",
    "        if (len(check_mem_feat)!=0) and (len(check_nmem_feat)!=0):\n",
    "        \n",
    "            check_mem_feat_idx =  np.array(check_mem_feat.index)\n",
    "\n",
    "\n",
    "            check_nmem_feat_idx =  np.array(check_nmem_feat.index)\n",
    "\n",
    "            #print(check_nmem_feat_idx)\n",
    "            #print(np.argmax(mpred,axis=1)==0)\n",
    "\n",
    "\n",
    "            mpred = attack_model.predict(np.array(check_mem_feat))    \n",
    "            predicted_membership.append(np.argmax(mpred,axis=1) )\n",
    "\n",
    "            nmpred = attack_model.predict(np.array(check_nmem_feat))    \n",
    "            predicted_nmembership.append(np.argmax(nmpred,axis=1) )        \n",
    "\n",
    "\n",
    "\n",
    "            TP_idx.append(check_mem_feat_idx[np.where(np.argmax(mpred,axis=1)==1)[0]])\n",
    "\n",
    "            TN_idx.append(check_nmem_feat_idx[np.where(np.argmax(nmpred,axis=1)==0)[0]])\n",
    "\n",
    "    pred_members = np.array([item for sublist in predicted_membership for item in sublist])\n",
    "    pred_nonmembers = np.array([item for sublist in predicted_nmembership for item in sublist])\n",
    "    \n",
    "    TP_idx_list = np.array([item for sublist in TP_idx for item in sublist])\n",
    "    TN_idx_list = np.array([item for sublist in TN_idx for item in sublist])\n",
    "    \n",
    "    members=np.array(list(pred_members))\n",
    "    nonmembers=np.array(list(pred_nonmembers))\n",
    "    \n",
    "    pred_membership = np.concatenate([members,nonmembers])\n",
    "    ori_membership = np.concatenate([np.ones(len(members)), np.zeros(len(nonmembers))])\n",
    "    \n",
    "    return pred_membership, ori_membership, TP_idx_list, TN_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-armenia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interracial-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(dataset , model_type, n_class, dim, channel):\n",
    "    if dataset in ('cifar10', 'cifar100') and model_type==\"Alexanet\":\n",
    "        \n",
    "        model, act_layer = build_alexanet(n_class,dim,channel) \n",
    "    \n",
    "    elif dataset in ('cifar10', 'cifar100') and model_type==\"VGG16\":\n",
    "         \n",
    "        model, act_layer = build_vgg (n_class,dim,channel)\n",
    "    \n",
    "    elif dataset==\"purchase100\" and model_type==\"DNN\":\n",
    "        \n",
    "        model, act_layer = buid_purchase_dnn(n_class,dim)\n",
    "    \n",
    "    \n",
    "    elif dataset==\"texas100\" and model_type==\"DNN\":   \n",
    "       \n",
    "        model, act_layer = buid_texas_dnn(n_class,dim)\n",
    "    \n",
    "    elif dataset==\"location\" and model_type==\"DNN\":   \n",
    "       \n",
    "        model, act_layer = buid_location_dnn(n_class,dim)\n",
    "\n",
    "    elif dataset in ('MNIST','FMNIST') and model_type==\"Alexanet\": #MLP\n",
    "         \n",
    "        model, act_layer = build_simple_cnn (n_class,dim,channel)\n",
    "        #model, act_layer = build_alexanet(n_class,dim,channel) \n",
    "\n",
    "    elif dataset in ('adult') and model_type==\"MLP\":\n",
    "         \n",
    "        model, act_layer = build_simple_mlp (n_class,dim,channel)\n",
    "                    \n",
    "    elif dataset in ('synthetic') and model_type==\"MLP\":\n",
    "         \n",
    "        model, act_layer = build_simple_mlp (n_class,dim,channel)\n",
    "     \n",
    "    return model, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brilliant-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buid_purchase_dnn(n_class,dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(600, input_dim=dim))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    #model.add(Dense(1024), kernel_regularizer=l2(0.001))\n",
    "    #model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(512, kernel_regularizer=l2(0.00003)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "\n",
    "    model.add(Dense(256, kernel_regularizer=l2(0.00003)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(128, kernel_regularizer=l2(0.00003)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    #opt = SGD(lr=0.01, momentum=0.9)\n",
    "    #model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    act_layer=6\n",
    "    \n",
    "    return model, act_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spare-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buid_texas_dnn(n_class,dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(1024, input_dim=dim, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(512, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(256, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "\n",
    "    model.add(Dense(128, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "       \n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    act_layer=6\n",
    "    \n",
    "    return model, act_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "individual-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buid_location_dnn(n_class,dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(512, input_dim=dim, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(248, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(128, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "\n",
    "    model.add(Dense(64, kernel_regularizer=l2(0.0007)))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "       \n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    act_layer=6\n",
    "    \n",
    "    return model, act_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "checked-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn(n_class,pix,d):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(pix, pix, d)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32,  kernel_initializer='he_uniform'))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    act_layer=3\n",
    "    \n",
    "    return model, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "right-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_mlp(n_class,pix,d):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=pix))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(256, kernel_regularizer=l2(0.01)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "    \n",
    "    \n",
    "    #model.add(Dense(248))\n",
    "    #model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.01))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(n_class, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    act_layer=3\n",
    "    \n",
    "    return model, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "talented-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_mlp(pix,d):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=pix))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    #model.add(Dropout(0.1))\n",
    "\n",
    "#     model.add(Dense(32))\n",
    "#     model.add(Activation(\"tanh\"))\n",
    "#     model.add(Dropout(0.01))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    act_layer=1\n",
    "    \n",
    "    return model, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "certified-router",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(1,28,28), data_format='channels_first'))\n",
    "# AlexNet model\n",
    "class AlexNet(Sequential):\n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(96, kernel_size=(11,11), strides= 4,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        input_shape= input_shape, \n",
    "                        kernel_initializer= 'he_normal'))\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'same', data_format= None))\n",
    "\n",
    "        self.add(Conv2D(256, kernel_size=(5,5), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'same',  data_format= None)) \n",
    "\n",
    "        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(Conv2D(384, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(Conv2D(256, kernel_size=(3,3), strides= 1,\n",
    "                        padding= 'same', activation= 'relu',\n",
    "                        kernel_initializer= 'he_normal'))\n",
    "\n",
    "        self.add(MaxPooling2D(pool_size=(3,3), strides= (2,2),\n",
    "                              padding= 'same', data_format= None))\n",
    "\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(4096, activation= 'relu'))\n",
    "        self.add(Dense(4096, activation= 'relu'))\n",
    "        self.add(Dense(1000, activation= 'relu'))\n",
    "        self.add(Dense(num_classes, activation= 'softmax'))\n",
    "\n",
    "        self.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "optimum-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_alexanet (n_class,pix,d):\n",
    "    \n",
    "        \n",
    "    model = AlexNet((pix, pix, d), n_class)\n",
    "    \n",
    "    act_layer=11\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    return model, act_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "golden-groove",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n"
     ]
    }
   ],
   "source": [
    "####load full dataset#### ****\n",
    "dataset='FMNIST' #'MNIST', 'cifar10', 'cifar100', 'purchase100', 'texas100' 'FMNIST' #location #synthetic\n",
    "model_type='Alexanet'  #Alexanet #VGG16 #DNN #CNN #MLP\n",
    "trainX_full, trainY_full, num_classes, dim = load_dataset_full(dataset,model_type) \n",
    " \n",
    "if dataset in ('MNIST', 'FMNIST', 'cifar10', 'cifar100'):\n",
    "    trainX_full = prep_pixels_full(trainX_full)\n",
    "\n",
    "\n",
    "#####unbalanced classes########\n",
    "trainX=trainX_full\n",
    "trainY=trainY_full\n",
    "\n",
    "\n",
    "####load completely different dataet####\n",
    "if dataset == 'cifar10':\n",
    "    dd_dataset='cifar10'\n",
    "    per_class_sample=2000\n",
    "    channel=3\n",
    "    EPS=200\n",
    "    act_layer=11\n",
    "    \n",
    "elif dataset == 'MNIST':\n",
    "    dd_dataset='FMNIST'\n",
    "    per_class_sample=2000\n",
    "    channel=1\n",
    "    EPS=10#0\n",
    "    act_layer=4\n",
    "    \n",
    "elif dataset == 'cifar100':\n",
    "    dd_dataset='cifar100'\n",
    "    per_class_sample=100\n",
    "    channel=3\n",
    "    EPS=100\n",
    "    act_layer=11\n",
    "    \n",
    "elif dataset == 'purchase100':\n",
    "    dd_dataset='purchase100'\n",
    "    per_class_sample=150\n",
    "    channel=0\n",
    "    EPS=50\n",
    "    act_layer=6\n",
    "    \n",
    "elif dataset == 'texas100':\n",
    "    dd_dataset='texas100'\n",
    "    per_class_sample=100\n",
    "    channel=0    \n",
    "    EPS=8\n",
    "    num_classes=100\n",
    "    act_layer=6\n",
    "    \n",
    "elif dataset == 'adult':\n",
    "    dd_dataset='adult'\n",
    "    per_class_sample=5000\n",
    "    channel=0   \n",
    "    EPS=200\n",
    "    act_layer=3\n",
    "    \n",
    "elif dataset == 'FMNIST':\n",
    "    dd_dataset='MNIST'\n",
    "    per_class_sample=2000\n",
    "    channel=1 \n",
    "    EPS=100\n",
    "    act_layer=4\n",
    "    \n",
    "elif dataset == 'location':\n",
    "    dd_dataset='location'\n",
    "    per_class_sample=40\n",
    "    channel=1 \n",
    "    EPS=100\n",
    "    act_layer=6\n",
    "    \n",
    "elif dataset == 'synthetic':\n",
    "    dd_dataset='synthetic'\n",
    "    per_class_sample=2500\n",
    "    channel=1 \n",
    "    EPS=10\n",
    "    \n",
    "init_model,_= define_model(dataset , model_type, num_classes, dim, channel)\n",
    "init_weights = init_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cloudy-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRZ=per_class_sample*num_classes\n",
    "OUTZ=TRZ\n",
    "SHZ=int(TRZ*0.5)\n",
    "TEZ=OUTZ\n",
    "\n",
    "\n",
    "\n",
    "itp, ifp, ifn, itn, iprecision, iadvj, iacc, irecall = [], [], [], [], [], [], [], []\n",
    "\n",
    "me_iprecision, me_iacc, me_recall, me_train_acc, me_test_acc = [], [], [], [], []\n",
    "e_iprecision, e_iacc, e_recall, e_train_acc, e_test_acc = [], [], [], [], []\n",
    "conf_iprecision, conf_iacc, conf_recall, conf_train_acc, conf_test_acc = [], [], [], [], []\n",
    "cc_precision, cc_recall, cc_acc = [],[],[]\n",
    "\n",
    "\n",
    "i_train_acc=[]\n",
    "i_test_acc=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "statutory-aaron",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "#########################Normal training ##################################\n",
    "\n",
    "#######training data#######\n",
    "IND_x_train=trainX\n",
    "IND_y_train=trainY\n",
    "               \n",
    "               \n",
    "##########get training data from inliers with balanced classes##########\n",
    "if dataset not in ['purchase100']:\n",
    "    tr_idx=list(range(0,IND_x_train.shape[0]))\n",
    "    idx = sample_selection(IND_x_train,IND_y_train,per_class_sample,tr_idx)\n",
    "    idx = random.sample(idx,len(idx))\n",
    "    idx = np.array([item for sublist in idx for item in sublist])\n",
    "    \n",
    "else:\n",
    "    #####un-balanced classes########\n",
    "    tot_class_samples = 20000\n",
    "    tr_idx=list(range(0,IND_x_train.shape[0]))\n",
    "    idx = random.sample(tr_idx,tot_class_samples)\n",
    "\n",
    "    TRZ=tot_class_samples\n",
    "    OUTZ=TRZ\n",
    "    SHZ=int(TRZ*0.5)\n",
    "    TEZ=OUTZ\n",
    "\n",
    "    \n",
    "x_train=trainX[idx]\n",
    "y_train=trainY[idx]               \n",
    "\n",
    "t_size = x_train.shape[0]\n",
    "\n",
    "#####  shuffel data  #########\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "\n",
    "remaining_inliers=set(tr_idx)-set(idx)\n",
    "           \n",
    "##########get testing data from inliers##########               \n",
    "\n",
    "test_idx=random.sample(set(remaining_inliers), TEZ)\n",
    "remaining_inliers_ii=set(remaining_inliers)-set(test_idx)      \n",
    "\n",
    "x_test=trainX[test_idx]\n",
    "y_test=trainY[test_idx]\n",
    "\n",
    "##########get shadow data from inliers##########               \n",
    "shadow_idx=random.sample(set(remaining_inliers_ii), SHZ)\n",
    "\n",
    "x_shadow=trainX[shadow_idx]\n",
    "y_shadow=trainY[shadow_idx]\n",
    "\n",
    "#########for creating noisy data ###############\n",
    "remaining_inliers_iii=set(remaining_inliers_ii)-set(shadow_idx)      \n",
    "noisy_idx = remaining_inliers_iii\n",
    "\n",
    "\n",
    "##########get ood test data from inliers##########               \n",
    "\n",
    "#ood_x_test=trainX[outliers]\n",
    "#ood_y_test=trainY[outliers]\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-university",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "greek-flavor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRZ :  20000  TEZ :  20000  SHZ :  10000 DD :  20000\n"
     ]
    }
   ],
   "source": [
    "##########get dd data ##########               \n",
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "noise = 1\n",
    "\n",
    "if dd_dataset in ('MNIST', 'FMNIST'):\n",
    "    trainX_full_cood, trainY_full_cood, num_classes_cood, dimentionality_cood = load_dataset_full(dd_dataset)\n",
    "    dd_x_test=trainX_full_cood\n",
    "    dd_y_test=trainY_full_cood \n",
    "\n",
    "    #dd_x_test=trainX[list(noisy_idx)]\n",
    "    #dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "    #dd_x_test = dd_noisy_img_creation_28pix(dd_x_test)\n",
    "    \n",
    "    dd_x_test = dd_noisy_img_creation_sandp(dd_x_test)\n",
    "    dd_x_test = np.array(dd_x_test)\n",
    "\n",
    "    \n",
    "elif dd_dataset in ('cifar10', 'cifar100'):\n",
    "    dd_x_test=trainX[list(noisy_idx)]\n",
    "    dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "    dd_x_test = dd_noisy_img_creation(dd_x_test)\n",
    "    dd_x_test = np.array(dd_x_test)\n",
    "    \n",
    "elif dd_dataset in ('purchase100'):\n",
    "    noisy_idx = list(noisy_idx)[0:20000]\n",
    "    dd_x_test=trainX[list(noisy_idx)]\n",
    "    dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "    #dd_x_test = dd_noisy_bin_data_binnoise(dd_x_test)\n",
    "    dd_x_test = dd_noisy_bin_data(dd_x_test,noise)\n",
    "    \n",
    "elif dd_dataset in ('texas100'):\n",
    "    noisy_idx = list(noisy_idx)[0:10000]\n",
    "    dd_x_test=trainX[list(noisy_idx)]\n",
    "    dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "    #dd_x_test = dd_noisy_bin_data_binnoise(dd_x_test)\n",
    "    dd_x_test = dd_noisy_bin_data(dd_x_test,noise)\n",
    "    \n",
    "elif dd_dataset in ('location'):\n",
    "    dd_x_test=trainX[list(noisy_idx)]\n",
    "    dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "    #dd_x_test = dd_noisy_bin_data_binnoise(dd_x_test)\n",
    "    dd_x_test = dd_noisy_bin_data(dd_x_test,noise)\n",
    "    \n",
    "elif dd_dataset in ('adult'):\n",
    "#     dd_x_test=trainX[list(noisy_idx)]\n",
    "#     dd_y_test=trainY[list(noisy_idx)]\n",
    "\n",
    "#     dd_x_test = dd_noisy_bin_data(dd_x_test,noise)\n",
    "    \n",
    "    \n",
    "    trainX_full_cood, _, _, _ = load_dataset_full('purchase100')\n",
    "    trainX_full_cood = pd.DataFrame(trainX_full_cood)\n",
    "    \n",
    "    expected_dim = trainX_full.shape[1]\n",
    "    expected_instances = trainX_full_cood.shape[0]\n",
    "\n",
    "    dd_x_test = trainX_full_cood.iloc[:,1:15]\n",
    "    dd_x_test = np.array(dd_x_test)\n",
    "    dd_y_test = [randint(0, 1) for p in range(0, expected_instances)]\n",
    "    dd_y_test = np.array(dd_y_test)\n",
    "    dd_y_test = to_categorical(dd_y_test)\n",
    "\n",
    "\n",
    "    #dd_x_test=trainX[list(noisy_idx)]\n",
    "    #dd_y_test=trainY[list(noisy_idx)]\n",
    "    \n",
    "    dd_x_test = pd.DataFrame(dd_x_test)\n",
    "    dd_x_test.iloc[:,2]=dd_x_test.iloc[:,2]+100\n",
    "    #dd_x_test.iloc[:,0]=0\n",
    "\n",
    "elif dd_dataset in ('synthetic'):\n",
    "    dd_x_test=trainX[list(noisy_idx)]\n",
    "    dd_y_test=trainY[list(noisy_idx)]\n",
    "    dd_y_test=np.argmax(dd_y_test,axis=1)\n",
    "    dd_y_test = np.where(dd_y_test==1,0,1)   \n",
    "    dd_y_test = to_categorical(dd_y_test)\n",
    "\n",
    "    \n",
    "#size of OOD data\n",
    "dd_x_test = dd_x_test[0:t_size]\n",
    "dd_y_test = dd_y_test[0:t_size]\n",
    "\n",
    "print(\"TRZ : \",TRZ, \" TEZ : \", TEZ ,\" SHZ : \", SHZ, \"DD : \", dd_x_test.shape[0])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-chassis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-ordinary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-champagne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hourly-scratch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shadow Model  0\n",
      "Shadow Train acc :  100.0 Shadow Test acc :  86.59999966621399\n",
      "Shadow Model  0\n",
      "Shadow Train acc :  100.0 Shadow Test acc :  80.26000261306763\n",
      "Training Acc :  1.0  IND Test Acc :  0.89445  OOD Test Acc :  0.89445  DD Test Acc :  0.0823\n"
     ]
    }
   ],
   "source": [
    "#shadow train vs shdow test\n",
    "n_shadow_train_performance, n_shadow_test_performance, n_attack_data, x_shadow_train, y_shadow_train, x_shadow_test, y_shadow_test, shadow_model_init = shadow_model(x_shadow, y_shadow, dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "#shadow train vs shdow test\n",
    "noisy_shadow_train_performance, noisy_shadow_test_performance, noisy_attack_data, noisy_x_shadow_train, noisy_y_shadow_train, noisy_x_shadow_test, noisy_y_shadow_test, noisy_shadow_model_init = shadow_model_noisy(x_shadow, y_shadow, dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "\n",
    "#train model\n",
    "n_ind_train_acc, n_ind_test_acc, n_histories, n_BTR, n_BTE, n_ind_test_performance, n_ind_train_performance, n_tm_model, n_target_ood_performance, n_target_dd_performance, pred_status = evaluate_model(x_train, y_train, x_test, y_test,x_test, y_test, dd_x_test, dd_y_test, dataset, model_type, num_classes, dim, channel, init_weights)\n",
    "\n",
    "i_train_acc.append(n_ind_train_acc)\n",
    "i_test_acc.append(n_ind_test_acc)\n",
    "\n",
    "##########################   inlier outlier identification ##########################\n",
    "ori_test_y_tm = n_tm_model.predict(x_test, verbose=0)\n",
    "inliers = np.where(np.argmax(y_test,axis=1)== np.argmax(ori_test_y_tm,axis=1))[0]\n",
    "outliers = np.where(np.argmax(y_test,axis=1)!= np.argmax(ori_test_y_tm,axis=1))[0]\n",
    "####################################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "disturbed-question",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Normal random/split member/nonmember data noisy shadow-------------\n",
      "non-member size :  20000\n",
      "member size :  20000\n",
      "For membership inference attack via correctness, the attack acc is 0.553, with train acc 1.000 and test acc 0.894\n",
      "TP :  20000\n",
      "TN :  2111\n",
      "FN :  0\n",
      "FP :  17889\n",
      "precision :  0.5278576895668927\n",
      "recall :  1.0\n",
      "acc :  0.552775\n",
      "confidence\n",
      "TP :  19090\n",
      "TN :  4049\n",
      "FN :  910\n",
      "FP :  15951\n",
      "precision :  0.5447903884021574\n",
      "recall :  0.9545\n",
      "acc :  0.578475\n",
      "For membership inference attack via confidence, the attack acc is 0.578\n",
      "entropy\n",
      "TP :  18867\n",
      "TN :  3407\n",
      "FN :  1133\n",
      "FP :  16593\n",
      "precision :  0.5320642978003384\n",
      "recall :  0.94335\n",
      "acc :  0.55685\n",
      "For membership inference attack via entropy, the attack acc is 0.557\n",
      "modified entropy\n",
      "TP :  19002\n",
      "TN :  4143\n",
      "FN :  998\n",
      "FP :  15857\n",
      "precision :  0.5451103015003299\n",
      "recall :  0.9501\n",
      "acc :  0.578625\n",
      "For membership inference attack via modified entropy, the attack acc is 0.579\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Normal random/split member/nonmember data noisy shadow-------------\")\n",
    "\n",
    "print(\"non-member size : \", len(n_ind_test_performance[0]))\n",
    "print(\"member size : \", len(n_ind_test_performance[0]))\n",
    "         \n",
    "MIA = black_box_benchmarks(noisy_shadow_train_performance,noisy_shadow_test_performance,\n",
    "                             n_ind_train_performance,n_ind_test_performance,n_tm_model,num_classes=num_classes)\n",
    "NOISYMIIND=MIA._mem_inf_benchmarks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "convertible-series",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Normal random/split member/nonmember data - Modified entropy MIA-------------\n",
      "non-member size :  20000\n",
      "member size :  20000\n",
      "For membership inference attack via correctness, the attack acc is 0.553, with train acc 1.000 and test acc 0.894\n",
      "TP :  20000\n",
      "TN :  2111\n",
      "FN :  0\n",
      "FP :  17889\n",
      "precision :  0.5278576895668927\n",
      "recall :  1.0\n",
      "acc :  0.552775\n",
      "confidence\n",
      "TP :  19951\n",
      "TN :  3500\n",
      "FN :  49\n",
      "FP :  16500\n",
      "precision :  0.5473375216043456\n",
      "recall :  0.99755\n",
      "acc :  0.586275\n",
      "For membership inference attack via confidence, the attack acc is 0.586\n",
      "entropy\n",
      "TP :  19765\n",
      "TN :  2691\n",
      "FN :  235\n",
      "FP :  17309\n",
      "precision :  0.533122943302584\n",
      "recall :  0.98825\n",
      "acc :  0.5614\n",
      "For membership inference attack via entropy, the attack acc is 0.561\n",
      "modified entropy\n",
      "TP :  19934\n",
      "TN :  3525\n",
      "FN :  66\n",
      "FP :  16475\n",
      "precision :  0.5475019912658958\n",
      "recall :  0.9967\n",
      "acc :  0.586475\n",
      "For membership inference attack via modified entropy, the attack acc is 0.586\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----------Normal random/split member/nonmember data - Modified entropy MIA-------------\")\n",
    "\n",
    "print(\"non-member size : \", len(n_ind_test_performance[0]))\n",
    "print(\"member size : \", len(n_ind_train_performance[0]))\n",
    "         \n",
    "MIA = black_box_benchmarks(n_shadow_train_performance,n_shadow_test_performance,\n",
    "                             n_ind_train_performance,n_ind_test_performance,n_tm_model,num_classes=num_classes)\n",
    "NOMIIND=MIA._mem_inf_benchmarks()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abandoned-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct classification statistics\n",
    "#train_pred_y\n",
    "Y_train_pred = n_tm_model.predict_classes(x_train, verbose=0)\n",
    "Y_train_pred_score = n_tm_model.predict(x_train, verbose=0)\n",
    "cc_tp = np.where(Y_train_pred==n_ind_train_performance[1])\n",
    "cc_fn = np.where(Y_train_pred!=n_ind_train_performance[1])\n",
    "\n",
    "\n",
    "#test_pred_y\n",
    "Y_test_pred = n_tm_model.predict_classes(x_test, verbose=0)\n",
    "Y_test_pred_score = n_tm_model.predict(x_test, verbose=0)\n",
    "cc_tn = np.where(Y_test_pred!=n_ind_test_performance[1])\n",
    "cc_fp = np.where(Y_test_pred==n_ind_test_performance[1])\n",
    "\n",
    "cc_tp=cc_tp[0]\n",
    "cc_fn=cc_fn[0]\n",
    "cc_tn=cc_tn[0]\n",
    "cc_fp=cc_fp[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sorted-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ################### max confidence score extraction ###########################\n",
    "\n",
    "ind_train_prob, ind_test_prob = extract_prob(n_ind_train_performance[0],n_ind_test_performance[0])\n",
    "_, ood_test_prob = extract_prob(n_target_dd_performance[0], n_target_dd_performance[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-burden",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "upset-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a94b68d50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a94b299d0>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a94b335d0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a94b33f50>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a94b38e90>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a94b11390>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a94b68d50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a94b299d0>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a94b335d0>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a94b33f50>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a94b38e90>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a94b11390>]\n",
      "Train :  Counter({1: 17601, -1: 2399})\n",
      "Test :  Counter({1: 17548, -1: 2452})\n"
     ]
    }
   ],
   "source": [
    "# #####neigbouthood based outliers #####\n",
    "# #unique class vals\n",
    "# class_val=np.unique(np.argmax(y_train))\n",
    "\n",
    "# #membership inference\n",
    "train_act  = extract_activation(n_tm_model,x_train,y_train,act_layer)\n",
    "test_act  = extract_activation(n_tm_model,x_test,y_test,act_layer)\n",
    "\n",
    "outlier_df = pd.concat([train_act,test_act],axis=0)\n",
    "outlier_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "clf = LocalOutlierFactor(n_neighbors=1)\n",
    "outlier_staus_train = clf.fit_predict(train_act)\n",
    "outlier_staus_test = clf.fit_predict(test_act)\n",
    "\n",
    "print(\"Train : \",Counter(outlier_staus_train))\n",
    "print(\"Test : \",Counter(outlier_staus_test))\n",
    "\n",
    "tr_knnoutlier_idx = np.where(outlier_staus_train==-1)[0]\n",
    "te_knnoutlier_idx = np.where(outlier_staus_test==-1)[0]\n",
    "\n",
    "tr_knninlier_idx = np.where(outlier_staus_train==1)[0]\n",
    "te_knninlier_idx = np.where(outlier_staus_test==1)[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fabulous-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### confidence score based outlier ######\n",
    "# ind_train_prob, ind_test_prob = extract_prob(n_ind_train_performance[0],n_ind_test_performance[0])\n",
    "\n",
    "# #outliers and inliers\n",
    "# thresh=0.6\n",
    "# conf_outlier_indices = np.where(ind_test_prob <= thresh)[0]\n",
    "# conf_inlier_indices = np.where(ind_test_prob > thresh)[0]\n",
    "\n",
    "# #get outliers and inliers predictions\n",
    "# conf_outlier_pred_score = n_ind_test_performance[0][conf_outlier_indices]\n",
    "# conf_inlier_pred_score = n_ind_test_performance[0][conf_inlier_indices]\n",
    "\n",
    "# #get outliers and inliers classes\n",
    "# conf_outlier_class = n_ind_test_performance[1][conf_outlier_indices]\n",
    "# conf_inlier_class = n_ind_test_performance[1][conf_inlier_indices]\n",
    "\n",
    "# #concatanate arrays\n",
    "# conf_outliers=(conf_outlier_pred_score,conf_outlier_class)\n",
    "# conf_inliers=(conf_inlier_pred_score,conf_inlier_class)\n",
    "\n",
    "# #Equal datasizes - nonmembers\n",
    "# Eq_outlier, Eq_inlier = data_extraction(conf_outliers,conf_inliers)\n",
    "\n",
    "# #Equal datasizes - train data\n",
    "# Eq_train, _ = data_extraction(n_ind_train_performance,Eq_outlier)\n",
    "\n",
    "# #Equal datasizes - test data\n",
    "# _, Eq_test = data_extraction(Eq_train,n_ind_test_performance)\n",
    "\n",
    "# #Equal datasizes - OOD data\n",
    "# _, Eq_DD = data_extraction(Eq_train,n_target_dd_performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-chaos",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hungarian-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########correct classification based outliers/inliers############\n",
    "\n",
    "#get outliers and inliers on correct classification predictions\n",
    "cc_outlier_pred_score = n_ind_test_performance[0][cc_tn]\n",
    "cc_inlier_pred_score = n_ind_test_performance[0][cc_fp]\n",
    "\n",
    "#get outliers and inliers classes\n",
    "cc_outlier_class = n_ind_test_performance[1][cc_tn]\n",
    "cc_inlier_class = n_ind_test_performance[1][cc_fp]\n",
    "\n",
    "\n",
    "#concatanate arrays\n",
    "cc_outliers=(cc_outlier_pred_score,cc_outlier_class)\n",
    "cc_inliers=(cc_inlier_pred_score,cc_inlier_class)\n",
    "\n",
    "\n",
    "# #Equal datasizes - train data\n",
    "Eq_train, _ = data_extraction(n_ind_train_performance,cc_outliers)\n",
    "\n",
    "#Equal datasizes - CC inlier data\n",
    "_, Eq_inlier = data_extraction(Eq_train,cc_inliers)\n",
    "\n",
    "\n",
    "#Equal datasizes - IC outlier data\n",
    "_, Eq_outlier = data_extraction(Eq_train,cc_outliers)\n",
    "\n",
    "# #Equal datasizes - OOD data\n",
    "_, Eq_DD = data_extraction(Eq_train,n_target_dd_performance)\n",
    "\n",
    "# #Equal datasizes - test data\n",
    "_, Eq_test = data_extraction(Eq_train,n_ind_test_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "collectible-bleeding",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------MIA---------------\n",
      "\n",
      "\n",
      "\n",
      "Normal\n",
      "\n",
      "\n",
      "\n",
      "For membership inference attack via correctness, the attack acc is 0.551, with train acc 1.000 and test acc 0.899\n",
      "TP :  2111\n",
      "TN :  214\n",
      "FN :  0\n",
      "FP :  1897\n",
      "precision :  0.5266966067864272\n",
      "recall :  1.0\n",
      "acc :  0.5506868782567503\n",
      "confidence\n",
      "TP :  2104\n",
      "TN :  375\n",
      "FN :  7\n",
      "FP :  1736\n",
      "precision :  0.5479166666666667\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.5871624822359072\n",
      "For membership inference attack via confidence, the attack acc is 0.587\n",
      "entropy\n",
      "TP :  2086\n",
      "TN :  289\n",
      "FN :  25\n",
      "FP :  1822\n",
      "precision :  0.5337768679631525\n",
      "recall :  0.9881572714353387\n",
      "acc :  0.5625296068214116\n",
      "For membership inference attack via entropy, the attack acc is 0.563\n",
      "modified entropy\n",
      "TP :  2104\n",
      "TN :  377\n",
      "FN :  7\n",
      "FP :  1734\n",
      "precision :  0.5482021886399167\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.5876361913784937\n",
      "For membership inference attack via modified entropy, the attack acc is 0.588\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OOD\n",
      "\n",
      "\n",
      "\n",
      "For membership inference attack via correctness, the attack acc is 0.957, with train acc 1.000 and test acc 0.086\n",
      "TP :  2111\n",
      "TN :  1930\n",
      "FN :  0\n",
      "FP :  181\n",
      "precision :  0.9210296684118674\n",
      "recall :  1.0\n",
      "acc :  0.9571293225959261\n",
      "confidence\n",
      "TP :  2104\n",
      "TN :  1930\n",
      "FN :  7\n",
      "FP :  181\n",
      "precision :  0.9207877461706784\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.9554713405968736\n",
      "For membership inference attack via confidence, the attack acc is 0.955\n",
      "entropy\n",
      "TP :  2086\n",
      "TN :  4\n",
      "FN :  25\n",
      "FP :  2107\n",
      "precision :  0.4974958263772955\n",
      "recall :  0.9881572714353387\n",
      "acc :  0.4950260540028423\n",
      "For membership inference attack via entropy, the attack acc is 0.495\n",
      "modified entropy\n",
      "TP :  2104\n",
      "TN :  1930\n",
      "FN :  7\n",
      "FP :  181\n",
      "precision :  0.9207877461706784\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.9554713405968736\n",
      "For membership inference attack via modified entropy, the attack acc is 0.955\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### MIA - Classification probability Outliers #######################\n",
    "print(\"--------MIA---------------\")\n",
    "\n",
    "train_data_list=[Eq_train, Eq_train]\n",
    "test_data_list=[Eq_test, Eq_DD]\n",
    "\n",
    "\n",
    "    \n",
    "ddconf_iprecision, ddconf_iacc, ddconf_recall, dde_iprecision, dde_iacc, dde_recall, ddme_iprecision, ddme_iacc,ddme_recall, ddcc_precision, ddcc_acc,ddcc_recall = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "for i in range(len(train_data_list)):\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    if i == 0:\n",
    "        print(\"Normal\")\n",
    "    elif i ==1:\n",
    "        print(\"OOD\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    MIA = black_box_benchmarks(n_shadow_train_performance,n_shadow_test_performance,\n",
    "                             train_data_list[i],test_data_list[i],n_tm_model,num_classes=num_classes)\n",
    "    ENMEM=MIA._mem_inf_benchmarks()\n",
    "\n",
    "\n",
    "    \n",
    "    ddconf_iprecision.append(ENMEM[6])\n",
    "    ddconf_iacc.append(ENMEM[7])\n",
    "    ddconf_recall.append(ENMEM[8])\n",
    "    #print(\"\\n\")\n",
    "\n",
    "\n",
    "    dde_iprecision.append(ENMEM[15])\n",
    "    dde_iacc.append(ENMEM[16])\n",
    "    dde_recall.append(ENMEM[17])\n",
    "    #print(\"\\n\")\n",
    "\n",
    "\n",
    "    ddme_iprecision.append(ENMEM[24])\n",
    "    ddme_iacc.append(ENMEM[25])\n",
    "    ddme_recall.append(ENMEM[26])\n",
    "\n",
    "\n",
    "    ddcc_precision.append(ENMEM[30] )\n",
    "    ddcc_acc.append(ENMEM[31] )\n",
    "    ddcc_recall.append(ENMEM[32])\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "instructional-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@@@@\n",
    "# softmax_prob_train, softmax_prob_test = extract_prob(train_data_list[0][0],test_data_list[0][0])\n",
    "\n",
    "# _, softmax_prob_DD = extract_prob(train_data_list[1][0],test_data_list[1][0])\n",
    "\n",
    "# print(np.mean(softmax_prob_train), np.std(softmax_prob_train))\n",
    "# print(np.mean(softmax_prob_test), np.std(softmax_prob_test))\n",
    "# print(np.mean(softmax_prob_DD), np.std(softmax_prob_DD))\n",
    "\n",
    "# #sys.exit(\"Stop here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "outdoor-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# fig, ax = plt.subplots(sharey=True)\n",
    "# sns.distplot( softmax_prob_train , color=\"orange\", hist=False, label=\"Members\",ax=ax)\n",
    "# sns.distplot( softmax_prob_test , color=\"dodgerblue\", hist=False, kde_kws={'linestyle':'--'}, label=\"Non-Members (IND)\",ax=ax)\n",
    "# sns.distplot( softmax_prob_DD , color=\"deeppink\", hist=False, kde_kws={'linestyle':':'}, label=\"Non-Members (OOD/ Noisy)\",ax=ax)\n",
    "\n",
    "\n",
    "# plt.legend();\n",
    "# plt.xlabel(\"Max soft-max confidence values\")\n",
    "\n",
    "# # ax2 = plt.axes([0.2, 0.22, .4, .4], facecolor='w')\n",
    "# # sns.distplot( softmax_prob_train , color=\"orange\", hist=False, label=\"Members\",ax=ax2)\n",
    "# # sns.distplot( softmax_prob_test , color=\"dodgerblue\", hist=False, kde_kws={'linestyle':'--'}, label=\"Non-Members (IND)\",ax=ax2)\n",
    "# # sns.distplot( softmax_prob_DD , color=\"deeppink\", hist=False, kde_kws={'linestyle':':'}, label=\"Non-Members (OOD/ Noisy)\",ax=ax2)\n",
    "\n",
    "# # ax2.set_xlim([0.6,1.04])\n",
    "# # ax2.set_ylim([0,25])\n",
    "# # ax2.set(xlabel='',ylabel='')\n",
    "\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "# fig.savefig(img_path+dataset+'_noisy.png', dpi=200) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "greek-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #shadow model\n",
    "\n",
    "# #set threshold\n",
    "# shadow_tr_max, shadow_te_max = extract_prob(n_shadow_train_performance[0], n_shadow_test_performance[0])\n",
    "# thr = thre_setting(shadow_tr_max,shadow_te_max)\n",
    "\n",
    "# #membership\n",
    "# tr_max, te_max = extract_prob(Eq_train[0], Eq_test[0])\n",
    "# new_pred_mem = np.where(tr_max > thr,1,0) \n",
    "# new_pred_nmem = np.where(te_max >thr,1,0)\n",
    "\n",
    "# full_new_pred  = np.concatenate((new_pred_mem,new_pred_nmem))\n",
    "# full_new_ori = np.concatenate( (np.ones(len(new_pred_mem)), np.zeros(len(new_pred_nmem))))\n",
    "\n",
    "# tn, fp, fn, tp = confusion_matrix(full_new_ori, full_new_pred).ravel()\n",
    "\n",
    "# print(\"TP : \", tp )\n",
    "# print(\"TN : \", tn)\n",
    "# print(\"FN : \", fn )\n",
    "# print(\"FP : \", fp,\"\\n\")                                  \n",
    "\n",
    "# precision=tp/(tp+fp)\n",
    "# recall=tp/(tp+fn)\n",
    "# acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "# print(\"precision : \",precision)\n",
    "# print(\"recall : \",recall)\n",
    "# print(\"acc : \", acc)                                  \n",
    "# print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "lasting-space",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "knowing-phoenix",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------MIA---------------\n",
      "\n",
      "\n",
      "\n",
      "Random\n",
      "\n",
      "\n",
      "\n",
      "For membership inference attack via correctness, the attack acc is 0.551, with train acc 1.000 and test acc 0.899\n",
      "TP :  2111\n",
      "TN :  214\n",
      "FN :  0\n",
      "FP :  1897\n",
      "precision :  0.5266966067864272\n",
      "recall :  1.0\n",
      "acc :  0.5506868782567503\n",
      "confidence\n",
      "TP :  2104\n",
      "TN :  375\n",
      "FN :  7\n",
      "FP :  1736\n",
      "precision :  0.5479166666666667\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.5871624822359072\n",
      "For membership inference attack via confidence, the attack acc is 0.587\n",
      "entropy\n",
      "TP :  2086\n",
      "TN :  289\n",
      "FN :  25\n",
      "FP :  1822\n",
      "precision :  0.5337768679631525\n",
      "recall :  0.9881572714353387\n",
      "acc :  0.5625296068214116\n",
      "For membership inference attack via entropy, the attack acc is 0.563\n",
      "modified entropy\n",
      "TP :  2104\n",
      "TN :  377\n",
      "FN :  7\n",
      "FP :  1734\n",
      "precision :  0.5482021886399167\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.5876361913784937\n",
      "For membership inference attack via modified entropy, the attack acc is 0.588\n",
      "\n",
      "\n",
      "\n",
      "Inlier NM + CC\n",
      "\n",
      "\n",
      "\n",
      "For membership inference attack via correctness, the attack acc is 0.500, with train acc 1.000 and test acc 1.000\n",
      "TP :  2111\n",
      "TN :  0\n",
      "FN :  0\n",
      "FP :  2111\n",
      "precision :  0.5\n",
      "recall :  1.0\n",
      "acc :  0.5\n",
      "confidence\n",
      "TP :  2104\n",
      "TN :  153\n",
      "FN :  7\n",
      "FP :  1958\n",
      "precision :  0.517971442639094\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.534580767408811\n",
      "For membership inference attack via confidence, the attack acc is 0.535\n",
      "entropy\n",
      "TP :  2086\n",
      "TN :  173\n",
      "FN :  25\n",
      "FP :  1938\n",
      "precision :  0.518389662027833\n",
      "recall :  0.9881572714353387\n",
      "acc :  0.5350544765513975\n",
      "For membership inference attack via entropy, the attack acc is 0.535\n",
      "modified entropy\n",
      "TP :  2104\n",
      "TN :  157\n",
      "FN :  7\n",
      "FP :  1954\n",
      "precision :  0.5184820108427797\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.5355281856939839\n",
      "For membership inference attack via modified entropy, the attack acc is 0.536\n",
      "\n",
      "\n",
      "\n",
      "Outlier NM + ICC\n",
      "\n",
      "\n",
      "\n",
      "For membership inference attack via correctness, the attack acc is 1.000, with train acc 1.000 and test acc 0.000\n",
      "TP :  2111\n",
      "TN :  2111\n",
      "FN :  0\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  1.0\n",
      "acc :  1.0\n",
      "confidence\n",
      "TP :  2104\n",
      "TN :  2111\n",
      "FN :  7\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.9983420180009475\n",
      "For membership inference attack via confidence, the attack acc is 0.998\n",
      "entropy\n",
      "TP :  2086\n",
      "TN :  1156\n",
      "FN :  25\n",
      "FP :  955\n",
      "precision :  0.6859585662610983\n",
      "recall :  0.9881572714353387\n",
      "acc :  0.7678825201326386\n",
      "For membership inference attack via entropy, the attack acc is 0.768\n",
      "modified entropy\n",
      "TP :  2104\n",
      "TN :  2111\n",
      "FN :  7\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.9966840360018948\n",
      "acc :  0.9983420180009475\n",
      "For membership inference attack via modified entropy, the attack acc is 0.998\n"
     ]
    }
   ],
   "source": [
    "####################### MIA - Classification probability based methods #######################\n",
    "print(\"--------MIA---------------\")\n",
    "\n",
    "#train_data_list=[n_ind_train_performance,Eq_train,Eq_train,Eq_train,Eq_train,Eq_train,Eq_train]\n",
    "#test_data_list=[n_ind_test_performance,Eq_test,Eq_inlier,Eq_outlier,Eq_DD,Eq_CC_inlier,Eq_CC_outlier]\n",
    "\n",
    "train_data_list=[Eq_train, Eq_train,     Eq_train]\n",
    "test_data_list=[ Eq_test,  Eq_inlier, Eq_outlier]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(train_data_list)):\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    if i == 0:\n",
    "        print(\"Random\")\n",
    "    elif i ==1:\n",
    "        print(\"Inlier NM + CC\")\n",
    "    elif i ==2:\n",
    "        print(\"Outlier NM + ICC\")\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    \n",
    "    MIA = black_box_benchmarks(n_shadow_train_performance,n_shadow_test_performance,\n",
    "                             train_data_list[i],test_data_list[i],n_tm_model,num_classes=num_classes)\n",
    "    ENMEM=MIA._mem_inf_benchmarks()\n",
    "\n",
    "\n",
    "    \n",
    "    conf_iprecision.append(ENMEM[6])\n",
    "    conf_iacc.append(ENMEM[7])\n",
    "    conf_recall.append(ENMEM[8])\n",
    "\n",
    "\n",
    "    e_iprecision.append(ENMEM[15])\n",
    "    e_iacc.append(ENMEM[16])\n",
    "    e_recall.append(ENMEM[17])\n",
    "\n",
    "\n",
    "    me_iprecision.append(ENMEM[24])\n",
    "    me_iacc.append(ENMEM[25])\n",
    "    me_recall.append(ENMEM[26])\n",
    "\n",
    "    per_class_threshold = ENMEM[27]\n",
    "\n",
    "    true_membership = ENMEM[28]\n",
    "    pred_membership = ENMEM[29]\n",
    "\n",
    "\n",
    "    cc_precision.append(ENMEM[30] )\n",
    "    cc_acc.append(ENMEM[31] )\n",
    "    cc_recall.append(ENMEM[32])\n",
    "\n",
    "    if i == 0:\n",
    "        TP_CONF = ENMEM[0]\n",
    "        TP_ENTR = ENMEM[9]\n",
    "        TP_MENTR = ENMEM[18]\n",
    "        \n",
    "        TN_CONF = ENMEM[3]\n",
    "        TN_ENTR = ENMEM[12]\n",
    "        TN_MENTR = ENMEM[21]\n",
    "        \n",
    "        CC_TP = ENMEM[33]\n",
    "        CC_FN = ENMEM[34]\n",
    "        CC_TN = ENMEM[35]\n",
    "        CC_FP = ENMEM[36]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # # ################### max confidence score extraction ###########################\n",
    "\n",
    "        softmax_prob_train, softmax_prob_test = extract_prob(train_data_list[0][0],test_data_list[0][0])\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-secondary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-insured",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sunrise-trinidad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIA - logloss attack\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "loss_tresh_list :  {0: -0.0, 1: -0.0, 2: -0.0, 3: -0.0, 4: -0.0, 5: -0.0, 6: -0.0, 7: -0.0, 8: -0.0, 9: -0.0}\n",
      "TP :  0\n",
      "TN :  20000\n",
      "FN :  20000\n",
      "FP :  0\n",
      "precision :  nan\n",
      "recall :  0.0\n",
      "acc :  0.5\n",
      "\n",
      "\n",
      "\n",
      "TP :  0\n",
      "TN :  17889\n",
      "FN :  17889\n",
      "FP :  0\n",
      "precision :  nan\n",
      "recall :  0.0\n",
      "acc :  0.5\n",
      "\n",
      "\n",
      "\n",
      "TP :  0\n",
      "TN :  2111\n",
      "FN :  2111\n",
      "FP :  0\n",
      "precision :  nan\n",
      "recall :  0.0\n",
      "acc :  0.5\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/ipykernel/__main__.py:32: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "####################### MIA - logloss attack #######################\n",
    "print(\"MIA - logloss attack\")\n",
    "logloss_precision=[]\n",
    "logloss_acc=[]\n",
    "logloss_recall=[]\n",
    "\n",
    "\n",
    "### compute loglosss for shadow data\n",
    "shadow_train_instance_loss  = np.array(log_loss(n_shadow_train_performance[1],n_shadow_train_performance[0]))\n",
    "shadow_test_instance_loss  = np.array(log_loss(n_shadow_test_performance[1],n_shadow_test_performance[0]))\n",
    "\n",
    "#compute per class loss threshold values\n",
    "loss_tresh_list = loss_thre_setting(shadow_train_instance_loss, shadow_test_instance_loss, n_shadow_train_performance[1], n_shadow_test_performance[1])\n",
    "print(\"loss_tresh_list : \", loss_tresh_list)\n",
    "\n",
    "###get predictions for original membership validation data\n",
    "\n",
    "#prediction\n",
    "train_instance_pred_tm = n_tm_model.predict(x_train)\n",
    "test_instance_pred_tm = n_tm_model.predict(x_test)\n",
    "\n",
    "### compute loglosss for original data\n",
    "ori_train_instance_loss  = np.array(log_loss(np.argmax(y_train,axis=1),train_instance_pred_tm))\n",
    "ori_test_instance_loss  = np.array(log_loss(np.argmax(y_test,axis=1),test_instance_pred_tm))\n",
    "\n",
    "\n",
    "\n",
    "#random membership determination\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, ori_train_instance_loss, ori_test_instance_loss, np.argmax(train_instance_pred_tm,axis=1), np.argmax(test_instance_pred_tm,axis=1) )\n",
    "logloss_precision.append(prec)\n",
    "logloss_recall.append(rec)\n",
    "logloss_acc.append(acc)\n",
    "\n",
    "\n",
    "# #concatanate arrays\n",
    "ll_outliers=(ori_test_instance_loss[outliers],np.argmax(y_test,axis=1)[outliers])\n",
    "ll_inliers=(ori_test_instance_loss[inliers],np.argmax(y_test,axis=1)[inliers])\n",
    "ll_train=(ori_train_instance_loss,np.argmax(y_train,axis=1))\n",
    "\n",
    "\n",
    "# #Equal datasizes - CC inlier data\n",
    "Eq_TRA_inlier, Eq_TEST_inlier = data_extraction(ll_train,ll_inliers)\n",
    "\n",
    "\n",
    "# #Equal datasizes - IC outlier data\n",
    "Eq_TRA_outlier, Eq_TEST_outlier = data_extraction(ll_train,ll_outliers)\n",
    "\n",
    "\n",
    "#inlier memmbership determination\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, Eq_TRA_inlier[0], Eq_TEST_inlier[0], Eq_TRA_inlier[1], Eq_TEST_inlier[1])\n",
    "logloss_precision.append(prec)\n",
    "logloss_recall.append(rec)\n",
    "logloss_acc.append(acc)\n",
    "\n",
    "#outlier\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, Eq_TRA_outlier[0], Eq_TEST_outlier[0], Eq_TRA_outlier[1], Eq_TEST_outlier[1])\n",
    "logloss_precision.append(prec)\n",
    "logloss_recall.append(rec)\n",
    "logloss_acc.append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "collective-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIA - Label only transfer attack\n",
      "Shadow Model  0\n",
      "Shadow Train acc :  99.3399977684021 Shadow Test acc :  90.03999829292297\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "{0: 1.1920930376163597e-07, 1: 1.1920930376163597e-07, 2: 1.1920930376163597e-07, 3: 1.1920930376163597e-07, 4: 1.1920930376163597e-07, 5: 1.1920930376163597e-07, 6: 1.1920930376163597e-07, 7: 1.1920930376163597e-07, 8: 1.1920930376163597e-07, 9: 1.1920930376163597e-07}\n",
      "TP :  2347\n",
      "TN :  17743\n",
      "FN :  17653\n",
      "FP :  2257\n",
      "precision :  0.5097741094700261\n",
      "recall :  0.11735\n",
      "acc :  0.50225\n",
      "\n",
      "\n",
      "\n",
      "TP :  2115\n",
      "TN :  15632\n",
      "FN :  15774\n",
      "FP :  2257\n",
      "precision :  0.48376029277218663\n",
      "recall :  0.11822907932248868\n",
      "acc :  0.4960310805522947\n",
      "\n",
      "\n",
      "\n",
      "TP :  261\n",
      "TN :  2111\n",
      "FN :  1850\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.12363808621506395\n",
      "acc :  0.561819043107532\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### MIA - Label only transfer attack #######################\n",
    "print(\"MIA - Label only transfer attack\")\n",
    "trans_logloss_precision=[]\n",
    "trans_logloss_acc=[]\n",
    "trans_logloss_recall=[]\n",
    "\n",
    "### get target model classifications for shadow data\n",
    "shadow_pred_y = n_tm_model.predict(x_shadow, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "###train modified shadow model\n",
    "trans_shadow_train_performance, trans_shadow_test_performance, _, trans_x_shadow_train, trans_y_shadow_train, trans_x_shadow_test, trans_y_shadow_test, trans_shadow_model = shadow_model(x_shadow, shadow_pred_y, dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "### compute loglosss for shadow data\n",
    "shadow_train_instance_loss  = np.array(log_loss(trans_shadow_train_performance[1],trans_shadow_train_performance[0]))\n",
    "shadow_test_instance_loss  = np.array(log_loss(trans_shadow_test_performance[1],trans_shadow_test_performance[0]))\n",
    "\n",
    "#compute per class loss threshold values\n",
    "loss_tresh_list = loss_thre_setting(shadow_train_instance_loss, shadow_test_instance_loss, trans_shadow_train_performance[1], trans_shadow_test_performance[1])\n",
    "print(loss_tresh_list)\n",
    "\n",
    "###get predictions for original membership validation data\n",
    "\n",
    "#prediction\n",
    "train_instance_pred_sm = trans_shadow_model.predict(x_train)\n",
    "test_instance_pred_sm = trans_shadow_model.predict(x_test)\n",
    "\n",
    "\n",
    "### compute loglosss for shadow data\n",
    "ori_train_instance_loss  = np.array(log_loss(np.argmax(y_train,axis=1),train_instance_pred_sm))\n",
    "ori_test_instance_loss  = np.array(log_loss(np.argmax(y_test,axis=1),test_instance_pred_sm))\n",
    "\n",
    "\n",
    "#random memmbership determination\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, ori_train_instance_loss, ori_test_instance_loss, np.argmax(train_instance_pred_sm,axis=1), np.argmax(test_instance_pred_sm,axis=1) )\n",
    "trans_logloss_precision.append(prec)\n",
    "trans_logloss_recall.append(rec)\n",
    "trans_logloss_acc.append(acc)\n",
    "\n",
    "\n",
    "# #concatanate arrays\n",
    "trans_outliers=(ori_test_instance_loss[outliers],np.argmax(y_test,axis=1)[outliers])\n",
    "trans_inliers=(ori_test_instance_loss[inliers],np.argmax(y_test,axis=1)[inliers])\n",
    "trans_train=(ori_train_instance_loss,np.argmax(y_train,axis=1))\n",
    "\n",
    "# #train\n",
    "#shadow_train_loss = (shadow_train_instance_loss,mod_shadow_train_performance[1])\n",
    "\n",
    "\n",
    "# #Equal datasizes - CC inlier data\n",
    "Eq_TRA_inlier, Eq_TEST_inlier = data_extraction(trans_train,trans_inliers)\n",
    "\n",
    "\n",
    "# #Equal datasizes - IC outlier data\n",
    "Eq_TRA_outlier, Eq_TEST_outlier = data_extraction(trans_train,trans_outliers)\n",
    "\n",
    "\n",
    "#inlier memmbership determination\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, Eq_TRA_inlier[0], Eq_TEST_inlier[0], Eq_TRA_inlier[1], Eq_TEST_inlier[1])\n",
    "trans_logloss_precision.append(prec)\n",
    "trans_logloss_recall.append(rec)\n",
    "trans_logloss_acc.append(acc)\n",
    "\n",
    "#outlier\n",
    "prec, rec, acc = loss_mia (loss_tresh_list, Eq_TRA_outlier[0], Eq_TEST_outlier[0], Eq_TRA_outlier[1], Eq_TEST_outlier[1])\n",
    "trans_logloss_precision.append(prec)\n",
    "trans_logloss_recall.append(rec)\n",
    "trans_logloss_acc.append(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-poetry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-impact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "popular-extraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIA - Label only retrainining and activaion attack\n",
      "Mod shadow model - Train Acc:  0.88545 Test Acc :  0.8701\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "TP :  20000\n",
      "TN :  4184\n",
      "FN :  0\n",
      "FP :  15816\n",
      "precision :  0.5584096493187403\n",
      "recall :  1.0\n",
      "acc :  0.6046\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####################### MIA - shadow activation attack #######################\n",
    "# for validation data generate adverserial example\n",
    "\n",
    "print(\"MIA - Label only retrainining and activaion attack\")\n",
    "\n",
    "act_precision, act_recall, act_acc = [], [], []\n",
    "\n",
    "\n",
    "# #prediction-modsm\n",
    "_, tr_acc = trans_shadow_model.evaluate(x_train, y_train, verbose=0)\n",
    "_, te_acc = trans_shadow_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Mod shadow model - Train Acc: \", tr_acc, \"Test Acc : \",te_acc)\n",
    "\n",
    "#membership inference - valdation set\n",
    "train_act  = extract_activation(trans_shadow_model,x_train,y_train,act_layer)\n",
    "test_act  = extract_activation(trans_shadow_model,x_test,y_test,act_layer)\n",
    "\n",
    "#unique class vals\n",
    "class_val = np.unique(np.argmax(y_shadow,axis=1))\n",
    "\n",
    "#MIA\n",
    "act_df_full = pd.concat([train_act,test_act],axis=0)\n",
    "act_df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "true_membership = np.concatenate([np.ones(x_train.shape[0]), np.zeros(x_test.shape[0])])\n",
    "\n",
    "\n",
    "#membership inference - shadow set\n",
    "shdow_train_act  = extract_activation(trans_shadow_model,trans_x_shadow_train,trans_y_shadow_train,act_layer)\n",
    "shadow_test_act  = extract_activation(trans_shadow_model,trans_x_shadow_test, trans_y_shadow_test,act_layer)\n",
    "\n",
    "#MIA\n",
    "act_df_shadow = pd.concat([shdow_train_act,shadow_test_act],axis=0)\n",
    "act_df_shadow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "########## Random ###########\n",
    "\n",
    "nn=1\n",
    "pred_membership = activation_mia(act_df_full,class_val,act_layer,nn)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "print(\"TP : \", tp )\n",
    "print(\"TN : \", tn)\n",
    "print(\"FN : \", fn )\n",
    "print(\"FP : \", fp)\n",
    "        \n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "print(\"precision : \",precision)\n",
    "print(\"recall : \",recall)\n",
    "print(\"acc : \", acc)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "act_precision.append(precision)\n",
    "act_recall.append(recall)\n",
    "act_acc.append(acc) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dominant-terminology",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN   # kNN detector\n",
    "from pyod.models.abod import ABOD\n",
    "#0: inliers, 1: outliers\n",
    "\n",
    "def activation_mia_on_shadow(act_df_full,act_df_shadow, class_val,act_layer,nn):\n",
    "    \n",
    "    CLASS_IDX, dist_data, pred_mem, outlier, inlier = [], [], [], [], []\n",
    "\n",
    "    \n",
    "\n",
    "    outlier_fraction =0.5\n",
    "    for c_val in class_val:\n",
    "        print(c_val)\n",
    "\n",
    "        #validation data activation\n",
    "        filter_rec_all = act_df_full[(act_df_full['y'] == c_val)]\n",
    "        filter_rec_idx = np.array(filter_rec_all.index)\n",
    "        filter_rec=act_df_full.loc[filter_rec_idx]\n",
    "        filter_rec=filter_rec.loc[:, filter_rec.columns != 'y']\n",
    "\n",
    "        #shadow data activation\n",
    "        shadow_filter_rec_all = act_df_shadow[(act_df_shadow['y'] == c_val)]\n",
    "        shadow_filter_rec_idx = np.array(shadow_filter_rec_all.index)\n",
    "        shadow_filter_rec=act_df_shadow.loc[shadow_filter_rec_idx]\n",
    "        shadow_filter_rec=shadow_filter_rec.loc[:, shadow_filter_rec.columns != 'y']\n",
    "\n",
    "\n",
    "        clf = KNN(n_neighbors=nn)\n",
    "        #clf = KNN(contamination=outlier_fraction)\n",
    "        #clf = ABOD(contamination=outlier_fraction)\n",
    "   \n",
    "        clf.fit(shadow_filter_rec)\n",
    "\n",
    "        # predict raw anomaly score\n",
    "        scores_pred = clf.decision_function(shadow_filter_rec)*-1\n",
    "    \n",
    "        # prediction of a datapoint category outlier or inlier\n",
    "        filter_outlier = clf.predict(filter_rec)\n",
    "\n",
    "        filter_outlier_df = pd.DataFrame(filter_outlier)\n",
    "        filter_outlier_df = filter_outlier_df.set_index(filter_rec_idx)\n",
    "        filter_outlier_df.columns=['status'] \n",
    "\n",
    "        \n",
    "            \n",
    "        outlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 1]\n",
    "        outlier_idx_to_rec_idx = outlier_df.index.values\n",
    "        #print(outlier_idx_to_rec_idx)\n",
    "\n",
    "        inlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 0]    \n",
    "        #print(filter_inlier_df)\n",
    "        inlier_idx_to_rec_idx = inlier_df.index.values\n",
    "        #print(inlier_idx_to_rec_idx)\n",
    "\n",
    "        outlier.append(outlier_idx_to_rec_idx)\n",
    "        inlier.append(inlier_idx_to_rec_idx)\n",
    "        \n",
    "\n",
    "    nonmembers = np.array([item for sublist in outlier for item in sublist])\n",
    "    members = np.array([item for sublist in inlier for item in sublist])\n",
    "        \n",
    "    members=np.array(list(members))\n",
    "    nonmembers=np.array(list(nonmembers))\n",
    "        \n",
    "    pred_membership = np.concatenate([np.ones(len(members)), np.zeros(len(nonmembers))])\n",
    "    \n",
    "    outlier = np.array([item for sublist in outlier for item in sublist])\n",
    "    inlier = np.array([item for sublist in inlier for item in sublist])\n",
    "    \n",
    "    return pred_membership, outlier, inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "lyric-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  20000\n",
      "TN :  4699\n",
      "FN :  0\n",
      "FP :  15301\n",
      "precision :  0.5665561882099657\n",
      "recall :  1.0\n",
      "acc :  0.617475\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_membership, out_l, in_l = activation_mia_on_shadow(act_df_full, act_df_shadow, class_val,act_layer,10)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "\n",
    "print(\"TP : \", tp )\n",
    "print(\"TN : \", tn)\n",
    "print(\"FN : \", fn )\n",
    "print(\"FP : \", fp)\n",
    "\n",
    "\n",
    "precision=tp/(tp+fp)\n",
    "recall=tp/(tp+fn)\n",
    "acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "print(\"precision : \",precision)\n",
    "print(\"recall : \",recall)\n",
    "print(\"acc : \", acc)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "running-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data selection\n",
    "\n",
    "#original data\n",
    "tr_x = x_train\n",
    "tr_y = y_train\n",
    "\n",
    "te_x = x_test\n",
    "te_y = y_test\n",
    "\n",
    "v_feat = np.concatenate((tr_x,te_x))\n",
    "v_class = np.concatenate((tr_y,te_y)) \n",
    "\n",
    "#shadow data #relebelled\n",
    "s_tr_x = trans_x_shadow_train\n",
    "s_tr_y = trans_y_shadow_train\n",
    "\n",
    "s_te_x = trans_x_shadow_test\n",
    "s_te_y = trans_y_shadow_test\n",
    "\n",
    "s_v_feat = np.concatenate((s_tr_x,s_te_x))\n",
    "s_v_class = np.concatenate((s_tr_y,s_te_y)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "committed-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_based_MIA (trans_s_model, shadow_feat, shadow_class, val_feat, val_class, unique_label, act_layer, noise, dataset):\n",
    "\n",
    "    sd = noise \n",
    "    if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "        \n",
    "        mean = 0.0   # some constant\n",
    "        noisy_img = val_feat + np.random.normal(mean, sd, val_feat.shape)\n",
    "        noisy_x = np.clip(noisy_img, 0, 255)\n",
    "\n",
    "        #distance to original\n",
    "        dist_list=np.array(distf(val_feat, noisy_x,\"img\"))\n",
    "\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        noisy_x = dd_noisy_bin_data(val_feat,sd)\n",
    "\n",
    "        #distance to original\n",
    "        dist_list=np.array(distf(val_feat, noisy_x,\"-\"))\n",
    "\n",
    "        \n",
    "    #extract activation for noisy shdow data from shadow model#\n",
    "    noisy_act_df  = extract_activation(trans_s_model, noisy_x, val_class, act_layer)\n",
    "    noisy_act_df = pd.DataFrame(noisy_act_df)\n",
    "\n",
    "\n",
    "    #extract activation for membership validation data from shadow model#\n",
    "    shadow_act_df  = extract_activation(trans_s_model, shadow_feat, shadow_class, act_layer)\n",
    "    shadow_act_df = pd.DataFrame(shadow_act_df)\n",
    "    \n",
    "    sz = int(val_feat.shape[0]/2)\n",
    "    true_membership = np.concatenate([np.ones(sz), np.zeros(sz)])\n",
    "    \n",
    "    pred_membership, _, _ = activation_mia_on_shadow(noisy_act_df, shadow_act_df, class_val, act_layer, nn)\n",
    "   \n",
    "    tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "    print(\"TP : \", tp )\n",
    "    print(\"TN : \", tn)\n",
    "    print(\"FN : \", fn )\n",
    "    print(\"FP : \", fp)\n",
    "\n",
    "    precision=tp/(tp+fp)\n",
    "    recall=tp/(tp+fn)\n",
    "    acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "    print(\"precision : \",precision)\n",
    "    print(\"recall : \",recall)\n",
    "    print(\"acc : \", acc)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "#     shadow_act_precision.append(precision)\n",
    "#     shadow_act_recall.append(recall)\n",
    "#     shadow_act_acc.append(acc)     \n",
    "    \n",
    "     \n",
    "    return acc, precision, recall \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-deposit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "functional-lobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  5000\n",
      "TN :  1131\n",
      "FN :  0\n",
      "FP :  3869\n",
      "precision :  0.5637614161686774\n",
      "recall :  1.0\n",
      "acc :  0.6131\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  5000\n",
      "TN :  2336\n",
      "FN :  0\n",
      "FP :  2664\n",
      "precision :  0.6524008350730689\n",
      "recall :  1.0\n",
      "acc :  0.7336\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  4453\n",
      "TN :  5000\n",
      "FN :  547\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.8906\n",
      "acc :  0.9453\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  1076\n",
      "TN :  5000\n",
      "FN :  3924\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.2152\n",
      "acc :  0.6076\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  0\n",
      "TN :  5000\n",
      "FN :  5000\n",
      "FP :  0\n",
      "precision :  nan\n",
      "recall :  0.0\n",
      "acc :  0.5\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/ipykernel/__main__.py:43: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "##### Find noise parameter\n",
    "\n",
    "shadow_act_precision, shadow_act_recall, shadow_act_acc =[], [], [] \n",
    "\n",
    "\n",
    "MEMT, NONMEMT = [], []\n",
    "img_noise_range=[0.01, 0.1, 0.15, 0.2, 0.3]\n",
    "bin_noise_range=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "nn=10\n",
    "\n",
    "\n",
    "#unique class vals\n",
    "class_val = np.unique(np.argmax(s_v_class,axis=1))\n",
    "\n",
    "for t in range(5):\n",
    "    \n",
    "    print(t)    \n",
    "    \n",
    "    \n",
    "        \n",
    "    if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "        noise = img_noise_range[t]\n",
    "            \n",
    "    else:\n",
    "        noise = bin_noise_range[t]\n",
    "        \n",
    "    \n",
    "    acc, precision, recall  = activation_based_MIA (trans_shadow_model, s_tr_x, s_tr_y, s_v_feat, s_v_class, class_val, act_layer,noise, dataset)\n",
    "    \n",
    "\n",
    "    shadow_act_precision.append(precision)\n",
    "    shadow_act_recall.append(recall)\n",
    "    shadow_act_acc.append(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "planned-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####best noise value\n",
    "\n",
    "best_sd_list = np.add(np.array(shadow_act_precision)*0.5, np.array(shadow_act_acc)*0.5)\n",
    "best_sd_pos = np.nanargmax(best_sd_list)\n",
    "\n",
    "\n",
    "img_noise_range=[0.01, 0.1, 0.15, 0.2, 0.3]\n",
    "bin_noise_range=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "    noise = img_noise_range[best_sd_pos]\n",
    "else:\n",
    "    noise = bin_noise_range[best_sd_pos]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "declared-hazard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "worse-bones",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  16923\n",
      "TN :  20000\n",
      "FN :  3077\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.84615\n",
      "acc :  0.923075\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MEMT, NONMEMT = [], []\n",
    "nn=10\n",
    "\n",
    "new_act_precision, new_act_recall, new_act_acc =[], [], []\n",
    "\n",
    "acc, precision, recall  = activation_based_MIA (trans_shadow_model, s_tr_x, s_tr_y, v_feat, v_class, class_val, act_layer,noise, dataset)\n",
    "    \n",
    "\n",
    "\n",
    "new_act_precision.append(precision)\n",
    "new_act_recall.append(recall)\n",
    "new_act_acc.append(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-recycling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-filing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-representation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-utilization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-humidity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-berlin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-progressive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-scientist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "raising-bacon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  5000\n",
      "TN :  924\n",
      "FN :  0\n",
      "FP :  4076\n",
      "precision :  0.5509034817100044\n",
      "recall :  1.0\n",
      "acc :  0.5924\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  5000\n",
      "TN :  2001\n",
      "FN :  0\n",
      "FP :  2999\n",
      "precision :  0.6250781347668458\n",
      "recall :  1.0\n",
      "acc :  0.7001\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  4596\n",
      "TN :  5000\n",
      "FN :  404\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.9192\n",
      "acc :  0.9596\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  1020\n",
      "TN :  5000\n",
      "FN :  3980\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.204\n",
      "acc :  0.602\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  0\n",
      "TN :  5000\n",
      "FN :  5000\n",
      "FP :  0\n",
      "precision :  nan\n",
      "recall :  0.0\n",
      "acc :  0.5\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p37/lib/python3.7/site-packages/ipykernel/__main__.py:74: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# ##### Find noise parameter\n",
    "\n",
    "# shadow_act_precision, shadow_act_recall, shadow_act_acc =[], [], [] \n",
    "\n",
    "\n",
    "# MEMT, NONMEMT = [], []\n",
    "# img_noise_range=[0.01, 0.1, 0.15, 0.2, 0.3]\n",
    "# bin_noise_range=[0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# nn=10\n",
    "\n",
    "# for t in range(5):\n",
    "    \n",
    "#     print(t)    \n",
    "    \n",
    "#     #activation_based_MIA (s_model,feat,labels,unique_label,alact_layer,shadow_status)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "        \n",
    "#         mean = 0.0   # some constant\n",
    "#         sd = img_noise_range[t]   # some constant (standard deviation)\n",
    "#         noisy_img = s_v_feat + np.random.normal(mean, sd, s_v_feat.shape)\n",
    "#         noisy_x = np.clip(noisy_img, 0, 255)\n",
    "\n",
    "#         #distance to original\n",
    "#         dist_list=np.array(distf(s_v_feat, noisy_x,\"img\"))\n",
    "       \n",
    "            \n",
    "#     else:\n",
    "#         sd = bin_noise_range[t]\n",
    "#         noisy_x = dd_noisy_bin_data(s_v_feat,sd)\n",
    "          \n",
    "#         #distance to original\n",
    "#         dist_list=np.array(distf(s_v_feat, noisy_x,\"-\"))\n",
    "        \n",
    "        \n",
    "#     #membership inference - valdation set\n",
    "#     shadow_ori_train_act  = extract_activation(trans_shadow_model,noisy_x,s_v_class,act_layer)\n",
    "\n",
    "    \n",
    "    \n",
    "#     #unique class vals\n",
    "#     class_val = np.unique(np.argmax(s_v_class,axis=1))\n",
    "\n",
    "#     #MIA\n",
    "#     shadow_act_df_noisy = pd.DataFrame(shadow_ori_train_act)\n",
    "#     shadow_act_df_noisy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     true_membership = np.concatenate([np.ones(s_tr_x.shape[0]), np.zeros(s_te_x.shape[0])])\n",
    "\n",
    "\n",
    "#     #membership inference - shadow set\n",
    "#     shadow_train_act  = extract_activation(trans_shadow_model,trans_x_shadow_train,trans_y_shadow_train,act_layer)\n",
    "#     shadow_test_act  = extract_activation(trans_shadow_model,trans_x_shadow_test, trans_y_shadow_test,act_layer)\n",
    "\n",
    "#     #MIA\n",
    "#     ori_act_df_shadow = pd.concat([shadow_train_act,shadow_test_act],axis=0)\n",
    "#     ori_act_df_shadow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#     ########## Random ###########\n",
    "\n",
    "    \n",
    "    \n",
    "#     pred_membership, out_l, in_l = activation_mia_on_shadow(shadow_ori_train_act, ori_act_df_shadow, class_val, act_layer, nn)\n",
    "#     MEMT.append(in_l)\n",
    "#     NONMEMT.append(out_l)\n",
    "    \n",
    "#     tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "#     print(\"TP : \", tp )\n",
    "#     print(\"TN : \", tn)\n",
    "#     print(\"FN : \", fn )\n",
    "#     print(\"FP : \", fp)\n",
    "\n",
    "#     precision=tp/(tp+fp)\n",
    "#     recall=tp/(tp+fn)\n",
    "#     acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "#     print(\"precision : \",precision)\n",
    "#     print(\"recall : \",recall)\n",
    "#     print(\"acc : \", acc)\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "#     shadow_act_precision.append(precision)\n",
    "#     shadow_act_recall.append(recall)\n",
    "#     shadow_act_acc.append(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "regulation-logan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "appropriate-murray",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "Layer list : \n",
      "[<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f3a2038de50>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f3a20312d90>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f3a20300090>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a202ed510>, <tensorflow.python.keras.layers.core.Activation object at 0x7f3a202d3310>, <tensorflow.python.keras.layers.core.Dense object at 0x7f3a2025ff10>]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "TP :  3833\n",
      "TN :  20000\n",
      "FN :  16167\n",
      "FP :  0\n",
      "precision :  1.0\n",
      "recall :  0.19165\n",
      "acc :  0.595825\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MEMT, NONMEMT = [], []\n",
    "# max_t=1\n",
    "# nn=10\n",
    "# for t in range(max_t):\n",
    "#     print(t)    \n",
    "#     if dataset in ('MNIST', 'FMNIST', 'CIFAR10'):\n",
    "\n",
    "#         mean = 0.0   # some constant\n",
    "#         sd =0.2   # some constant (standard deviation)\n",
    "#         noisy_img = v_feat + np.random.normal(mean, sd, v_feat.shape)\n",
    "#         noisy_x = np.clip(noisy_img, 0, 255)\n",
    "\n",
    "#         #distance to original\n",
    "#         dist_list=np.array(distf(v_feat, noisy_x,\"img\"))\n",
    "       \n",
    "            \n",
    "#     else:\n",
    "#         noisy_x = dd_noisy_bin_data(v_feat,sd)\n",
    "          \n",
    "#         #distance to original\n",
    "#         dist_list=np.array(distf(v_feat, noisy_x,\"-\"))\n",
    "        \n",
    "        \n",
    "#     #membership inference - valdation set\n",
    "#     train_act  = extract_activation(trans_shadow_model,noisy_x,v_class,act_layer)\n",
    "\n",
    "    \n",
    "    \n",
    "#     #unique class vals\n",
    "#     class_val = np.unique(np.argmax(v_class,axis=1))\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_noisy = pd.DataFrame(train_act)\n",
    "#     act_df_noisy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     true_membership = np.concatenate([np.ones(tr_x.shape[0]), np.zeros(te_x.shape[0])])\n",
    "\n",
    "\n",
    "#     #membership inference - shadow set\n",
    "#     shadow_train_act  = extract_activation(trans_shadow_model,trans_x_shadow_train,trans_y_shadow_train,act_layer)\n",
    "#     shadow_test_act  = extract_activation(trans_shadow_model,trans_x_shadow_test, trans_y_shadow_test,act_layer)\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_ori_shadow = pd.concat([shadow_train_act,shadow_test_act],axis=0)\n",
    "#     act_df_ori_shadow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#     ########## Random ###########\n",
    "\n",
    "    \n",
    "    \n",
    "#     pred_membership, out_l, in_l = activation_mia_on_shadow(act_df_noisy, act_df_ori_shadow, class_val, act_layer, nn)\n",
    "#     MEMT.append(in_l)\n",
    "#     NONMEMT.append(out_l)\n",
    "    \n",
    "#     tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "#     print(\"TP : \", tp )\n",
    "#     print(\"TN : \", tn)\n",
    "#     print(\"FN : \", fn )\n",
    "#     print(\"FP : \", fp)\n",
    "\n",
    "#     precision=tp/(tp+fp)\n",
    "#     recall=tp/(tp+fn)\n",
    "#     acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "#     print(\"precision : \",precision)\n",
    "#     print(\"recall : \",recall)\n",
    "#     print(\"acc : \", acc)\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "#     act_precision.append(precision)\n",
    "#     act_recall.append(recall)\n",
    "#     act_acc.append(acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-quality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sufficient-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyod.models.knn import KNN   # kNN detector\n",
    "# from pyod.models.abod import ABOD\n",
    "# #0: inliers, 1: outliers\n",
    "\n",
    "# def activation_mia_on_shadow(act_df_full, act_df_shadow, class_val, act_layer, nn):\n",
    "    \n",
    "#     CLASS_IDX, dist_data, pred_mem, outlier, inlier = [], [], [], [], []\n",
    "\n",
    "    \n",
    "\n",
    "#     outlier_fraction =0.5\n",
    "#     for c_val in class_val:\n",
    "#         print(c_val)\n",
    "\n",
    "#         #validation data activation\n",
    "#         filter_rec_all = act_df_full[(act_df_full['y'] == c_val)]\n",
    "#         filter_rec_idx = np.array(filter_rec_all.index)\n",
    "#         filter_rec=act_df_full.loc[filter_rec_idx]\n",
    "#         filter_rec=filter_rec.loc[:, filter_rec.columns != 'y']\n",
    "\n",
    "#         #shadow data activation\n",
    "#         shadow_filter_rec_all = act_df_shadow[(act_df_shadow['y'] == c_val)]\n",
    "#         shadow_filter_rec_idx = np.array(shadow_filter_rec_all.index)\n",
    "#         shadow_filter_rec=act_df_shadow.loc[shadow_filter_rec_idx]\n",
    "#         shadow_filter_rec=shadow_filter_rec.loc[:, shadow_filter_rec.columns != 'y']\n",
    "\n",
    "\n",
    "#         #clf = KNN(n_neighbors=nn)\n",
    "#         clf = KNN(contamination=outlier_fraction)\n",
    "#         #clf = ABOD(contamination=outlier_fraction)\n",
    "   \n",
    "#         clf.fit(shadow_filter_rec)\n",
    "\n",
    "#         # predict raw anomaly score\n",
    "#         scores_pred = clf.decision_function(shadow_filter_rec)*-1\n",
    "\n",
    "#         # prediction of a datapoint category outlier or inlier\n",
    "#         filter_outlier = clf.predict(filter_rec)\n",
    "\n",
    "#         filter_outlier_df = pd.DataFrame(filter_outlier)\n",
    "#         filter_outlier_df = filter_outlier_df.set_index(filter_rec_idx)\n",
    "#         filter_outlier_df.columns=['status'] \n",
    "\n",
    "        \n",
    "#         #nonmembers    \n",
    "#         outlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 1]\n",
    "#         outlier_idx_to_rec_idx = outlier_df.index.values\n",
    "#         #print(outlier_idx_to_rec_idx)\n",
    "\n",
    "#         #members\n",
    "#         inlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 0]    \n",
    "#         #print(filter_inlier_df)\n",
    "#         inlier_idx_to_rec_idx = inlier_df.index.values\n",
    "#         #print(inlier_idx_to_rec_idx)\n",
    "\n",
    "#         outlier.append(outlier_idx_to_rec_idx)\n",
    "#         inlier.append(inlier_idx_to_rec_idx)\n",
    "        \n",
    "\n",
    "#     nonmembers = np.array([item for sublist in outlier for item in sublist])\n",
    "#     members = np.array([item for sublist in inlier for item in sublist])\n",
    "        \n",
    "#     members=np.array(list(members))\n",
    "#     nonmembers=np.array(list(nonmembers))\n",
    "        \n",
    "#     pred_membership = np.concatenate([np.ones(len(members)), np.zeros(len(nonmembers))])\n",
    "    \n",
    "#     outlier = np.array([item for sublist in outlier for item in sublist])\n",
    "#     inlier = np.array([item for sublist in inlier for item in sublist])\n",
    "    \n",
    "#     return pred_membership, outlier, inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "egyptian-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyod.models.knn import KNN   # kNN detector\n",
    "# from pyod.models.abod import ABOD\n",
    "# #0: inliers, 1: outliers\n",
    "\n",
    "# def activation_mia_on_shadow(act_df_full, act_df_shadow, class_val, act_layer, nn):\n",
    "    \n",
    "#     CLASS_IDX, dist_data, pred_mem, outlier, inlier = [], [], [], [], []\n",
    "\n",
    "    \n",
    "\n",
    "#     outlier_fraction =0.5\n",
    "#     for c_val in class_val:\n",
    "#         print(c_val)\n",
    "\n",
    "#         #validation data activation\n",
    "#         filter_rec_all = act_df_full[(act_df_full['y'] == c_val)]\n",
    "#         filter_rec_idx = np.array(filter_rec_all.index)\n",
    "#         filter_rec=act_df_full.loc[filter_rec_idx]\n",
    "#         filter_rec=filter_rec.loc[:, filter_rec.columns != 'y']\n",
    "\n",
    "#         #shadow data activation\n",
    "#         shadow_filter_rec_all = act_df_shadow[(act_df_shadow['y'] == c_val)]\n",
    "#         shadow_filter_rec_idx = np.array(shadow_filter_rec_all.index)\n",
    "#         shadow_filter_rec=act_df_shadow.loc[shadow_filter_rec_idx]\n",
    "#         shadow_filter_rec=shadow_filter_rec.loc[:, shadow_filter_rec.columns != 'y']\n",
    "\n",
    "\n",
    "#         #clf = KNN(n_neighbors=nn)\n",
    "#         clf = KNN(contamination=outlier_fraction)\n",
    "#         #clf = ABOD(contamination=outlier_fraction)\n",
    "   \n",
    "#         clf.fit(shadow_filter_rec)\n",
    "\n",
    "#         # predict raw anomaly score\n",
    "#         scores_pred = clf.decision_function(shadow_filter_rec)*-1\n",
    "\n",
    "#         # prediction of a datapoint category outlier or inlier\n",
    "#         filter_outlier = clf.predict(filter_rec)\n",
    "\n",
    "#         filter_outlier_df = pd.DataFrame(filter_outlier)\n",
    "#         filter_outlier_df = filter_outlier_df.set_index(filter_rec_idx)\n",
    "#         filter_outlier_df.columns=['status'] \n",
    "\n",
    "        \n",
    "#         #nonmembers    \n",
    "#         outlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 1]\n",
    "#         outlier_idx_to_rec_idx = outlier_df.index.values\n",
    "#         #print(outlier_idx_to_rec_idx)\n",
    "\n",
    "#         #members\n",
    "#         inlier_df = filter_outlier_df.loc[filter_outlier_df['status'] == 0]    \n",
    "#         #print(filter_inlier_df)\n",
    "#         inlier_idx_to_rec_idx = inlier_df.index.values\n",
    "#         #print(inlier_idx_to_rec_idx)\n",
    "\n",
    "#         outlier.append(outlier_idx_to_rec_idx)\n",
    "#         inlier.append(inlier_idx_to_rec_idx)\n",
    "        \n",
    "\n",
    "#     nonmembers = np.array([item for sublist in outlier for item in sublist])\n",
    "#     members = np.array([item for sublist in inlier for item in sublist])\n",
    "        \n",
    "#     members=np.array(list(members))\n",
    "#     nonmembers=np.array(list(nonmembers))\n",
    "        \n",
    "#     pred_membership = np.concatenate([np.ones(len(members)), np.zeros(len(nonmembers))])\n",
    "    \n",
    "#     outlier = np.array([item for sublist in outlier for item in sublist])\n",
    "#     inlier = np.array([item for sublist in inlier for item in sublist])\n",
    "    \n",
    "#     return pred_membership, outlier, inlier\n",
    "\n",
    "\n",
    "# #change the size of shadow model\n",
    "\n",
    "\n",
    "# ####################### MIA - Label only transfer attack #######################\n",
    "\n",
    "# sh_act_precision, sh_act_recall, sh_act_acc = [], [], []\n",
    "\n",
    "# #print(\"MIA - Label only transfer attack\")\n",
    "\n",
    "# #different shadow sizes\n",
    "# shadow_size = np.arange(200,x_shadow.shape[0],500)\n",
    "# shadow_size = np.concatenate((shadow_size, [x_shadow.shape[0]]))\n",
    "# shadow_size = [x_shadow.shape[0],x_shadow.shape[0]]\n",
    "\n",
    "# ### get target model classifications for shadow data\n",
    "# shadow_pred_y = n_tm_model.predict(x_shadow, verbose=0)\n",
    "\n",
    "# sh_idx=list(range(0,x_shadow.shape[0]))\n",
    "\n",
    "# tr_x = x_train[0:10]\n",
    "# tr_y = y_train[0:10]\n",
    "\n",
    "# te_x = x_test[0:200]\n",
    "# te_y = y_test[0:200]\n",
    "\n",
    "# MEM, NMEM = [], []\n",
    "\n",
    "\n",
    "# for sz in shadow_size:\n",
    "#     print(sz)\n",
    "    \n",
    "#     #ranomly select sz number of samples\n",
    "#     rand_sam = random.sample(sh_idx,sz)\n",
    "    \n",
    "#     #select random number of samples\n",
    "#     s_x = x_shadow[np.array(rand_sam)]\n",
    "#     s_y = shadow_pred_y[np.array(rand_sam)]\n",
    "#     s_y = y_shadow[np.array(rand_sam)]\n",
    "         \n",
    "    \n",
    "    \n",
    "#     ###train modified shadow model\n",
    "#     trans_shadow_train_performance, trans_shadow_test_performance,_, sx_train, sy_train, sx_test, sy_test, trans_shadow_model = shadow_model(s_x,s_y, dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "#     act_precision, act_recall, act_acc = [], [], []\n",
    "\n",
    "\n",
    "#     # #prediction-modsm\n",
    "#     _, tr_acc = trans_shadow_model.evaluate(tr_x, tr_y, verbose=0)\n",
    "#     _, te_acc = trans_shadow_model.evaluate(te_x, te_y, verbose=0)\n",
    "\n",
    "#     print(\"Mod shadow model - Train Acc: \", tr_acc, \"Test Acc : \",te_acc)\n",
    "\n",
    "#     #membership inference - valdation set\n",
    "#     train_act  = extract_activation(trans_shadow_model,tr_x,tr_y,act_layer)\n",
    "#     test_act  = extract_activation(trans_shadow_model,te_x,te_y,act_layer)\n",
    "\n",
    "#     #unique class vals\n",
    "#     class_val = np.unique(np.argmax(s_y,axis=1))\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_full = pd.concat([train_act,test_act],axis=0)\n",
    "#     act_df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     true_membership = np.concatenate([np.ones(tr_x.shape[0]), np.zeros(te_x.shape[0])])\n",
    "\n",
    "\n",
    "#     #membership inference - shadow set\n",
    "#     shdow_train_act  = extract_activation(trans_shadow_model,sx_train,sy_train,act_layer)\n",
    "#     shadow_test_act  = extract_activation(trans_shadow_model,sx_test, sy_test,act_layer)\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_shadow = pd.concat([shdow_train_act,shadow_test_act],axis=0)\n",
    "#     act_df_shadow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#     ########## Random ###########\n",
    "\n",
    "    \n",
    "#     pred_membership, nonmem, mem = activation_mia_on_shadow(act_df_full, act_df_shadow, class_val, act_layer, nn)\n",
    "#     MEM.append(mem)\n",
    "#     NMEM.append(nonmem)\n",
    "\n",
    "#     tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "#     print(\"TP : \", tp )\n",
    "#     print(\"TN : \", tn)\n",
    "#     print(\"FN : \", fn )\n",
    "#     print(\"FP : \", fp)\n",
    "\n",
    "#     precision=tp/(tp+fp)\n",
    "#     recall=tp/(tp+fn)\n",
    "#     acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "#     print(\"precision : \",precision)\n",
    "#     print(\"recall : \",recall)\n",
    "#     print(\"acc : \", acc)\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "#     sh_act_precision.append(precision)\n",
    "#     sh_act_recall.append(recall)\n",
    "#     sh_act_acc.append(acc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "electoral-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #change the size of shadow model\n",
    "\n",
    "\n",
    "# ####################### MIA - Label only transfer attack #######################\n",
    "\n",
    "# sh_act_precision, sh_act_recall, sh_act_acc = [], [], []\n",
    "\n",
    "# #print(\"MIA - Label only transfer attack\")\n",
    "\n",
    "# #different shadow sizes\n",
    "# shadow_size = np.arange(200,x_shadow.shape[0],500)\n",
    "# shadow_size = np.concatenate((shadow_size, [x_shadow.shape[0]]))\n",
    "\n",
    "\n",
    "# ### get target model classifications for shadow data\n",
    "# shadow_pred_y = n_tm_model.predict(x_shadow, verbose=0)\n",
    "\n",
    "# sh_idx=list(range(0,x_shadow.shape[0]))\n",
    "\n",
    "# tr_x = x_train[0:200]\n",
    "# tr_y = y_train[0:200]\n",
    "\n",
    "# te_x = x_test[0:200]\n",
    "# te_y = y_test[0:200]\n",
    "\n",
    "\n",
    "# MEM, NMEM = [], []\n",
    "\n",
    "\n",
    "# #for sz in shadow_size:\n",
    "# for sz in range(0,50):\n",
    "\n",
    "#     print(sz)\n",
    "    \n",
    "#     #ranomly select sz number of samples\n",
    "#     rand_sam = random.sample(sh_idx,200)\n",
    "    \n",
    "#     #select random number of samples\n",
    "#     s_x = x_shadow[np.array(rand_sam)]\n",
    "#     s_y = shadow_pred_y[np.array(rand_sam)]\n",
    "#     s_y = y_shadow[np.array(rand_sam)]\n",
    "         \n",
    "    \n",
    "    \n",
    "#     ###train modified shadow model\n",
    "#     trans_shadow_train_performance, trans_shadow_test_performance,_, sx_train, sy_train, sx_test, sy_test, trans_shadow_model = shadow_model(s_x,s_y, dataset, model_type, num_classes, dim, channel)\n",
    "\n",
    "#     act_precision, act_recall, act_acc = [], [], []\n",
    "\n",
    "\n",
    "#     # #prediction-modsm\n",
    "#     _, tr_acc = trans_shadow_model.evaluate(tr_x, tr_y, verbose=0)\n",
    "#     _, te_acc = trans_shadow_model.evaluate(te_x, te_y, verbose=0)\n",
    "\n",
    "#     print(\"Mod shadow model - Train Acc: \", tr_acc, \"Test Acc : \",te_acc)\n",
    "\n",
    "#     #membership inference - valdation set\n",
    "#     train_act  = extract_activation(trans_shadow_model,tr_x,tr_y,act_layer)\n",
    "#     test_act  = extract_activation(trans_shadow_model,te_x,te_y,act_layer)\n",
    "\n",
    "#     #unique class vals\n",
    "#     class_val = np.unique(np.argmax(s_y,axis=1))\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_full = pd.concat([train_act,test_act],axis=0)\n",
    "#     act_df_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     true_membership = np.concatenate([np.ones(tr_x.shape[0]), np.zeros(te_x.shape[0])])\n",
    "\n",
    "\n",
    "#     #membership inference - shadow set\n",
    "#     shdow_train_act  = extract_activation(trans_shadow_model,sx_train,sy_train,act_layer)\n",
    "#     shadow_test_act  = extract_activation(trans_shadow_model,sx_test, sy_test,act_layer)\n",
    "\n",
    "#     #MIA\n",
    "#     act_df_shadow = pd.concat([shdow_train_act,shadow_test_act],axis=0)\n",
    "#     act_df_shadow.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "#     ########## Random ###########\n",
    "\n",
    "    \n",
    "#     pred_membership, nonmem, mem = activation_mia_on_shadow(act_df_full, act_df_shadow, class_val, act_layer, nn)\n",
    "#     MEM.append(mem)\n",
    "#     NMEM.append(nonmem)\n",
    "\n",
    "#     tn, fp, fn, tp = confusion_matrix(true_membership, pred_membership).ravel()\n",
    "\n",
    "#     print(\"TP : \", tp )\n",
    "#     print(\"TN : \", tn)\n",
    "#     print(\"FN : \", fn )\n",
    "#     print(\"FP : \", fp)\n",
    "\n",
    "#     precision=tp/(tp+fp)\n",
    "#     recall=tp/(tp+fn)\n",
    "#     acc=(tp+tn)/(tp+tn+fn+fp)\n",
    "\n",
    "#     print(\"precision : \",precision)\n",
    "#     print(\"recall : \",recall)\n",
    "#     print(\"acc : \", acc)\n",
    "#     print(\"\\n\\n\")\n",
    "\n",
    "#     sh_act_precision.append(precision)\n",
    "#     sh_act_recall.append(recall)\n",
    "#     sh_act_acc.append(acc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "separate-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(sh_act_precision,marker=\"o\")\n",
    "# plt.plot(sh_act_acc,marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-integer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "contemporary-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_train_y_tm = np.argmax(n_tm_model.predict(x_train, verbose=0),axis=1)\n",
    "# pred_test_y_tm = np.argmax(n_tm_model.predict(x_test, verbose=0),axis=1)\n",
    "# all_pred = np.concatenate((pred_train_y_tm,pred_test_y_tm))\n",
    "\n",
    "# ori_train_y_tm = np.argmax(y_train,axis=1)\n",
    "# ori_test_y_tm = np.argmax(y_test,axis=1)\n",
    "# all_ori = np.concatenate((ori_train_y_tm,ori_test_y_tm))\n",
    "\n",
    "# all_correct_idx = np.where(all_pred==all_ori)[0]\n",
    "# all_incorrect_idx = np.where(all_pred!=all_ori)[0]\n",
    "\n",
    "# #pred non-members with incorrect classifications\n",
    "# ACT_IC_TN = len(intersection(out_l,all_incorrect_idx ))/ len(all_incorrect_idx)\n",
    "# ACT_IC_TP = len(intersection(in_l,all_correct_idx ))/ len(all_correct_idx)\n",
    "\n",
    "\n",
    "# print(\"TN : \", ACT_IC_TN)\n",
    "# print(\"TP :\", ACT_IC_TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "immune-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_train = np.concatenate((tr_x,te_x))\n",
    "\n",
    "\n",
    "# global_activation = [K.function([n_tm_model.input], \n",
    "#                               [layer.output])([np.array(part_train), 1])\n",
    "#                               for layer in n_tm_model.layers]\n",
    "\n",
    "\n",
    "# #extract activations for each layer\n",
    "# list_point=[]\n",
    "# for ly in range(len(global_activation)-1):\n",
    "#     #print(ly)\n",
    "#     data_point=[]\n",
    "#     for int in range(part_train.shape[0]):\n",
    "#         #print(int)\n",
    "#         arr1=global_activation[ly][0][int]\n",
    "#         data_point.append(arr1.ravel())\n",
    "#     list_point.append(data_point)\n",
    "    \n",
    "    \n",
    "# #organize activations per data instance\n",
    "\n",
    "# activation_array = []\n",
    "# for ins in range(0,part_train.shape[0]):\n",
    "#     print(ins)\n",
    "#     activation_array_tmp=[]\n",
    "#     for ly in range(len(list_point)):\n",
    "#         dz = list_point[ly][ins]\n",
    "#         activation_array_tmp.append(dz)\n",
    "#     activation_array_tmp = np.array(activation_array_tmp)\n",
    "#     activation_array_tmp = np.array([item for sublist in activation_array_tmp for item in sublist])\n",
    "#     activation_array.append(activation_array_tmp.ravel())\n",
    "\n",
    "# # mod_act = []\n",
    "# # for ins in range(0,part_train.shape[0]):\n",
    "# #     mod_act.append(np.where(activation_array[ins]>0, 1, 0))\n",
    "\n",
    "\n",
    "# # def n_activation(model,trainX,trainY,act_layer):\n",
    "# #     activation_list=[]\n",
    " \n",
    "# #     print(\"Layer list : \")\n",
    "# #     print(model.layers)\n",
    "    \n",
    "# #     global_activation = [K.function([model.input], \n",
    "# #                               [layer.output])([np.array(trainX), 1])\n",
    "# #                               for layer in model.layers]\n",
    "\n",
    "    \n",
    "# #     #print(pd.DataFrame(global_activation[act_layer][0]))\n",
    "# #     #print(pd.DataFrame(global_activation[act_layer]))\n",
    "    \n",
    "# #     global_Df = pd.DataFrame(global_activation[act_layer][0])\n",
    "    \n",
    "\n",
    "    \n",
    "# #     train_ix = list(range(trainY.shape[0]))\n",
    "# #     global_Df['y'] = np.argmax(trainY,axis=1)\n",
    "# #     class_val = np.unique(np.argmax(trainY,axis=1))\n",
    "# #     train_df = global_Df\n",
    "    \n",
    "# #     return train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-given",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-thread",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p37)",
   "language": "python",
   "name": "conda_tensorflow_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
